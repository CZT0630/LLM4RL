# maddpg/maddpg_agent.py
import torch
import torch.nn.functional as F
import numpy as np
from .maddpg_actor_critic import Actor, Critic
from .noise import OUNoise


class MADDPGAgent:
    def __init__(self, state_dim, action_dim, num_agents, agent_idx, config=None, max_action=None, **kwargs):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Agenté…ç½®
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.num_agents = num_agents
        self.agent_idx = agent_idx
        
        # å¤„ç†é…ç½®å‚æ•°
        if config is None:
            config = {}
        
        # ä»kwargsä¸­æå–å‚æ•°
        self.lr_actor = kwargs.get('lr_actor', config.get('lr_actor', 0.001))
        self.lr_critic = kwargs.get('lr_critic', config.get('lr_critic', 0.001))
        self.gamma = kwargs.get('gamma', config.get('gamma', 0.99))
        self.tau = kwargs.get('tau', config.get('tau', 0.01))
        self.batch_size = kwargs.get('batch_size', config.get('batch_size', 64))
        
        # ğŸ†• é€€ç«ç­–ç•¥å‚æ•°é…ç½®
        self.use_annealing = config.get('use_annealing', False)
        if self.use_annealing:
            # ä¸‰é˜¶æ®µé€€ç«ç­–ç•¥å‚æ•°
            self.initial_llm_distill_weight = config.get('initial_llm_distill_weight', 0.8)
            self.constant_llm_distill_weight = config.get('constant_llm_distill_weight', 0.15)
            self.final_llm_distill_weight = config.get('final_llm_distill_weight', 0.0)
            self.stage1_end_episode = config.get('stage1_end_episode', 300)
            self.stage2_end_episode = config.get('stage2_end_episode', 700)
            # å½“å‰è’¸é¦æƒé‡ï¼ˆåŠ¨æ€æ›´æ–°ï¼‰
            self.llm_distill_weight = self.initial_llm_distill_weight
        else:
            # ä½¿ç”¨å›ºå®šæƒé‡
            self.llm_distill_weight = config.get('llm_distill_weight', 0.1)
        
        # ç¡®å®šmax_action
        if max_action is None:
            max_action = num_agents + 2  # é»˜è®¤å€¼ï¼šè®¾å¤‡æ•°é‡ + è¾¹ç¼˜ + äº‘ç«¯
        self.max_action = max_action
        
        # ç½‘ç»œåˆå§‹åŒ–
        self.actor = Actor(state_dim, action_dim, max_action).to(self.device)
        self.actor_target = Actor(state_dim, action_dim, max_action).to(self.device)
        self.critic = Critic(state_dim, action_dim, num_agents).to(self.device)
        self.critic_target = Critic(state_dim, action_dim, num_agents).to(self.device)
        
        # å¤åˆ¶ç½‘ç»œå‚æ•°
        self.actor_target.load_state_dict(self.actor.state_dict())
        self.critic_target.load_state_dict(self.critic.state_dict())
        
        # ä¼˜åŒ–å™¨
        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=self.lr_actor)
        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=self.lr_critic)
        
        # å™ªå£°
        self.noise = OUNoise(action_dim)

        # è®­ç»ƒç»Ÿè®¡
        self.training_count = 0

    def select_action(self, state, add_noise=True, llm_advice=None):
        """é€‰æ‹©åŠ¨ä½œ"""
        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)

        # å¤„ç†LLMå»ºè®®
        if llm_advice is not None:
            llm_advice = torch.FloatTensor(llm_advice).unsqueeze(0).to(self.device)

        with torch.no_grad():
            action = self.actor(state, llm_advice).cpu().data.numpy().flatten()

        if add_noise:
            action += self.noise.sample()
            
        # ç¡®ä¿åŠ¨ä½œåœ¨æœ‰æ•ˆèŒƒå›´å†…
        action = np.clip(action, 0.0, 1.0)
        
        # å¤„ç†è¾¹ç¼˜æœåŠ¡å™¨IDï¼ˆæœ€åä¸€ä¸ªç»´åº¦ï¼‰
        if len(action) >= 2:
            # ä¿®æ”¹è¾¹ç¼˜æœåŠ¡å™¨é€‰æ‹©é€»è¾‘ï¼Œä½¿ç”¨æ›´å‡åŒ€çš„åˆ†å¸ƒ
            # 1. ä½¿ç”¨floorè€Œä¸æ˜¯ç®€å•çš„ä¹˜æ³•ï¼Œé¿å…æ€»æ˜¯åå‘é«˜å€¼
            # 2. æ·»åŠ éšæœºæ‰°åŠ¨ä»¥å¢åŠ å¤šæ ·æ€§
            edge_selection = action[-1]
            
            # æ–¹æ³•1: ä½¿ç”¨floorå‡½æ•°ï¼Œç¡®ä¿ä½å€¼ä¹Ÿæœ‰æœºä¼šè¢«é€‰ä¸­
            num_edges = 5  # è¾¹ç¼˜æœåŠ¡å™¨æ•°é‡
            edge_id = int(np.floor(edge_selection * num_edges))
            
            # æ–¹æ³•2: æ·»åŠ å°æ¦‚ç‡éšæœºé€‰æ‹©ï¼Œå¢åŠ æ¢ç´¢
            if np.random.random() < 0.2:  # 20%çš„æ¦‚ç‡éšæœºé€‰æ‹©
                edge_id = np.random.randint(0, num_edges)
                
            # ç¡®ä¿è¾¹ç¼˜æœåŠ¡å™¨IDåœ¨æœ‰æ•ˆèŒƒå›´å†…
            action[-1] = np.clip(edge_id, 0, num_edges - 1)

        return action

    def train(self, all_agents, shared_buffer):
        """
        è®­ç»ƒAgent - ä½¿ç”¨å…±äº«ç¼“å†²åŒº
        
        Args:
            all_agents: æ‰€æœ‰Agentçš„åˆ—è¡¨ï¼ˆç”¨äºè·å–å…¶ä»–Agentçš„åŠ¨ä½œï¼‰
            shared_buffer: å…±äº«ç»éªŒå›æ”¾ç¼“å†²åŒº
            
        Returns:
            dict: è®­ç»ƒç»“æœç»Ÿè®¡
        """
        # æ£€æŸ¥è¾“å…¥å‚æ•°
        if shared_buffer is None:
            raise ValueError(
                f"MADDPG Agent {self.agent_idx} è®­ç»ƒæ—¶å¿…é¡»æä¾›å…±äº«ç¼“å†²åŒºï¼"
                "MADDPGç®—æ³•è¦æ±‚æ‰€æœ‰Agentä½¿ç”¨ç›¸åŒçš„ç»éªŒå›æ”¾ç¼“å†²åŒºã€‚"
            )
        
        if all_agents is None:
            raise ValueError(
                f"MADDPG Agent {self.agent_idx} è®­ç»ƒæ—¶å¿…é¡»æä¾›æ‰€æœ‰Agentåˆ—è¡¨ï¼"
                "MADDPGç®—æ³•éœ€è¦å…¶ä»–Agentçš„ç½‘ç»œå‚æ•°è¿›è¡Œè®­ç»ƒã€‚"
            )
        
        # æ£€æŸ¥ç¼“å†²åŒºæ˜¯å¦æœ‰è¶³å¤Ÿçš„ç»éªŒ
        if len(shared_buffer) < self.batch_size:
            return {
                'message': f'ç¼“å†²åŒºæ ·æœ¬ä¸è¶³: {len(shared_buffer)} < {self.batch_size}',
                'skipped': True,
                'agent_id': self.agent_idx
            }
        
        # ä»å…±äº«ç¼“å†²åŒºé‡‡æ ·ç»éªŒ
        states, actions, rewards, next_states, dones, llm_actions = shared_buffer.sample(self.batch_size)
        
        # è½¬æ¢ä¸ºtensor
        states = torch.FloatTensor(states).to(self.device)
        actions = torch.FloatTensor(actions).to(self.device)
        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(self.device)
        next_states = torch.FloatTensor(next_states).to(self.device)
        dones = torch.BoolTensor(dones).unsqueeze(1).to(self.device)

        # è®­ç»ƒCriticç½‘ç»œ
        critic_loss = self._train_critic(states, actions, rewards, next_states, dones, all_agents)
        
        # è®­ç»ƒActorç½‘ç»œï¼ˆåŒ…å«LLMçŸ¥è¯†è’¸é¦ï¼‰
        actor_loss, distill_loss = self._train_actor(states, actions, llm_actions, all_agents)
        
        # è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
        self._soft_update(self.actor_target, self.actor, self.tau)
        self._soft_update(self.critic_target, self.critic, self.tau)
        
        self.training_count += 1
        
        return {
            'critic_loss': critic_loss.item(),
            'actor_loss': actor_loss.item(),
            'distill_loss': distill_loss.item() if isinstance(distill_loss, torch.Tensor) else distill_loss,
            'training_count': self.training_count,
            'buffer_size': len(shared_buffer),
            'agent_id': self.agent_idx
        }

    def _train_critic(self, states, actions, rewards, next_states, dones, all_agents):
        """è®­ç»ƒCriticç½‘ç»œ"""
        # è®¡ç®—ç›®æ ‡Qå€¼
        with torch.no_grad():
            # è·å–ä¸‹ä¸€çŠ¶æ€çš„åŠ¨ä½œï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…åº”è¯¥ç”¨å…¶ä»–Agentçš„ç›®æ ‡ç½‘ç»œï¼‰
            next_actions = []
            for i in range(self.num_agents):
                if i == self.agent_idx:
                    next_action = self.actor_target(next_states)
                else:
                    # ç®€åŒ–å¤„ç†ï¼šä½¿ç”¨å½“å‰Agentçš„ç›®æ ‡ç½‘ç»œä»£æ›¿å…¶ä»–Agent
                    # åœ¨å®Œæ•´å®ç°ä¸­ï¼Œè¿™é‡Œåº”è¯¥æ˜¯ all_agents[i].actor_target(next_states)
                    next_action = self.actor_target(next_states)
                next_actions.append(next_action)
            
            next_actions = torch.cat(next_actions, dim=1)
            
            # æ‰©å±•çŠ¶æ€ç»´åº¦ä»¥åŒ¹é…å¤šæ™ºèƒ½ä½“Criticè¾“å…¥
            states_expanded = states.repeat(1, self.num_agents)
            next_states_expanded = next_states.repeat(1, self.num_agents)
            
            next_q_values = self.critic_target(next_states_expanded, next_actions)
            target_q_values = rewards + self.gamma * next_q_values * (~dones)
        
        # å½“å‰Qå€¼
        current_actions = actions.repeat(1, self.num_agents)  # ç®€åŒ–ï¼šå‡è®¾æ‰€æœ‰AgentåŠ¨ä½œç›¸åŒ
        current_q_values = self.critic(states_expanded, current_actions)
        
        # CriticæŸå¤±
        critic_loss = F.mse_loss(current_q_values, target_q_values)
        
        # æ›´æ–°Critic
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()

        return critic_loss

    def _train_actor(self, states, actions, llm_actions, all_agents):
        """è®­ç»ƒActorç½‘ç»œï¼ˆåŒ…å«LLMçŸ¥è¯†è’¸é¦ï¼‰"""
        # ç­–ç•¥æ¢¯åº¦æŸå¤±
        predicted_actions = self.actor(states)
        
        # æ„å»ºç”¨äºCriticçš„åŠ¨ä½œåºåˆ—ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        actions_for_critic = predicted_actions.repeat(1, self.num_agents)
        states_expanded = states.repeat(1, self.num_agents)
        
        actor_loss = -self.critic(states_expanded, actions_for_critic).mean()
        
        # LLMçŸ¥è¯†è’¸é¦æŸå¤±
        distill_loss = self._compute_llm_distillation_loss(predicted_actions, llm_actions)
        
        # æ€»æŸå¤±
        total_actor_loss = actor_loss + self.llm_distill_weight * distill_loss
        
        # æ›´æ–°Actor
        self.actor_optimizer.zero_grad()
        total_actor_loss.backward()
        self.actor_optimizer.step()

        return total_actor_loss, distill_loss

    def _compute_llm_distillation_loss(self, predicted_actions, llm_actions):
        """è®¡ç®—LLMçŸ¥è¯†è’¸é¦æŸå¤±"""
        if not llm_actions or all(action is None for action in llm_actions):
            return torch.tensor(0.0, device=self.device)
        
        distill_loss = 0.0
        valid_samples = 0
        
        for i, llm_action in enumerate(llm_actions):
            if llm_action is not None and len(llm_action) > self.agent_idx:
                # è·å–è¯¥Agentå¯¹åº”çš„LLMä¸“å®¶åŠ¨ä½œ
                if isinstance(llm_action, list) and self.agent_idx < len(llm_action):
                    llm_agent_action = llm_action[self.agent_idx]
                elif isinstance(llm_action, (list, np.ndarray)) and len(llm_action) == self.action_dim:
                    llm_agent_action = llm_action
                else:
                    continue
                
                if llm_agent_action is not None and len(llm_agent_action) > 0:
                    llm_tensor = torch.FloatTensor(llm_agent_action).to(self.device)
                    # ç¡®ä¿ç»´åº¦åŒ¹é…
                    if llm_tensor.shape[-1] == predicted_actions.shape[-1]:
                        distill_loss += F.mse_loss(predicted_actions[i], llm_tensor)
                        valid_samples += 1
        
        if valid_samples > 0:
            return distill_loss / valid_samples
        else:
            return torch.tensor(0.0, device=self.device)

    def _soft_update(self, target, source, tau):
        """è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        for target_param, param in zip(target.parameters(), source.parameters()):
            target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)

    def get_training_info(self):
        """è·å–è®­ç»ƒä¿¡æ¯"""
        return {
            'agent_id': self.agent_idx,
            'training_count': self.training_count,
            'actor_lr': self.lr_actor,
            'critic_lr': self.lr_critic,
            'gamma': self.gamma,
            'tau': self.tau
        }

    def save_model(self, filepath):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'actor': self.actor.state_dict(),
            'critic': self.critic.state_dict(),
            'actor_target': self.actor_target.state_dict(),
            'critic_target': self.critic_target.state_dict(),
            'actor_optimizer': self.actor_optimizer.state_dict(),
            'critic_optimizer': self.critic_optimizer.state_dict(),
            'training_count': self.training_count,
        }, filepath)

    def load_model(self, filepath):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(filepath, map_location=self.device)
        self.actor.load_state_dict(checkpoint['actor'])
        self.critic.load_state_dict(checkpoint['critic'])
        self.actor_target.load_state_dict(checkpoint['actor_target'])
        self.critic_target.load_state_dict(checkpoint['critic_target'])
        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer'])
        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer'])
        self.training_count = checkpoint.get('training_count', 0)

    def update_llm_distill_weight(self, current_episode):
        """
        ğŸ†• ä¸‰é˜¶æ®µé€€ç«ç­–ç•¥æ›´æ–°LLMè’¸é¦æƒé‡
        
        Args:
            current_episode: å½“å‰è®­ç»ƒè½®æ•°ï¼ˆä»0å¼€å§‹ï¼‰
            
        Returns:
            float: æ›´æ–°åçš„è’¸é¦æƒé‡
        """
        if not self.use_annealing:
            return self.llm_distill_weight
            
        if current_episode <= self.stage1_end_episode:
            # é˜¶æ®µ1ï¼šçº¿æ€§é€€ç«ä»åˆå§‹æƒé‡åˆ°æ’å®šæƒé‡
            progress = current_episode / self.stage1_end_episode
            self.llm_distill_weight = (
                self.initial_llm_distill_weight * (1 - progress) + 
                self.constant_llm_distill_weight * progress
            )
        elif current_episode <= self.stage2_end_episode:
            # é˜¶æ®µ2ï¼šä¿æŒæ’å®šæƒé‡
            self.llm_distill_weight = self.constant_llm_distill_weight
        else:
            # é˜¶æ®µ3ï¼šå¿«é€Ÿé€€ç«åˆ°0
            total_stage3_episodes = self.stage2_end_episode + 100  # 100è½®å†…é™åˆ°0
            if current_episode <= total_stage3_episodes:
                progress = (current_episode - self.stage2_end_episode) / 100
                progress = min(progress, 1.0)
                self.llm_distill_weight = self.constant_llm_distill_weight * (1 - progress)
            else:
                self.llm_distill_weight = self.final_llm_distill_weight
        
        return self.llm_distill_weight

    def get_current_annealing_stage(self, current_episode):
        """
        ğŸ†• è·å–å½“å‰é€€ç«é˜¶æ®µä¿¡æ¯
        
        Args:
            current_episode: å½“å‰è®­ç»ƒè½®æ•°
            
        Returns:
            tuple: (é˜¶æ®µåç§°, é˜¶æ®µæè¿°)
        """
        if not self.use_annealing:
            return "å›ºå®šæƒé‡", f"å›ºå®šè’¸é¦æƒé‡ (æƒé‡: {self.llm_distill_weight:.3f})"
            
        if current_episode <= self.stage1_end_episode:
            return "é˜¶æ®µ1", f"ä¸“å®¶æŒ‡å¯¼é˜¶æ®µ (æƒé‡: {self.initial_llm_distill_weight:.2f} â†’ {self.constant_llm_distill_weight:.2f})"
        elif current_episode <= self.stage2_end_episode:
            return "é˜¶æ®µ2", f"å¹³è¡¡æ¢ç´¢é˜¶æ®µ (æƒé‡: {self.constant_llm_distill_weight:.2f})"
        else:
            return "é˜¶æ®µ3", f"è‡ªä¸»å­¦ä¹ é˜¶æ®µ (æƒé‡: {self.llm_distill_weight:.3f} â†’ 0.0)"