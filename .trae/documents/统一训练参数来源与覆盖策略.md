## 结论概述
- 参数来源统一为 `config.yaml` + 内置默认值的深度合并（缺省项自动补齐）。
- 各算法在各自训练脚本中读取对应段，并部分使用通用 `training` 段进行覆盖（如 episodes、保存/日志频率）。
- 环境 `CloudEdgeDeviceEnv` 目前以 `maddpg.max_steps` 控制内部截断步数，存在与非 MADDPG 算法不完全一致的风险。

## 当前设置来源
### 通用
- 配置加载与深度合并：`utils/config.py:4-15, 88-94`（`load_config` + `deep_update`）
- 随机种子：`utils/config.py:10-12`，各训练脚本 `set_seed` 调用（如 `experiments/maddpg/train_maddpg.py:15-23`）
- 通用训练策略：`config.yaml:195-226`（episodes、save_frequency、log_frequency、warm_up_episodes 等）
- LLM 服务参数：`config.yaml:163-183`，客户端读取 `llm_assistant/llm_client.py:28-38`

### 环境
- 设备规模等：`config.yaml:8-31`，实例化时读取：`environment/cloud_edge_env.py:54-57`
- 环境步数截断来源：`environment/cloud_edge_env.py:194-196`（优先使用 `maddpg.max_steps`，否则 `training.max_steps_per_episode`，默认 200）

### MADDPG
- 超参：`config.yaml:108-135`（lr、gamma、tau、buffer_size、batch_size、train_frequency、max_episodes、max_steps）
- 训练脚本读取与覆盖：
  - 环境：`experiments/maddpg/train_maddpg.py:47-48`
  - episodes 优先使用 `training.episodes`：`experiments/maddpg/train_maddpg.py:90-93`
  - 保存/日志/预热：`experiments/maddpg/train_maddpg.py:95-98`
  - 探索策略（预热+前70%）：`experiments/maddpg/train_maddpg.py:120-135`

### LLM+MADDPG（完整版）
- LLM 指导/蒸馏/退火等：`config.yaml:137-161`
- 训练参数读取：`experiments/llm_maddpg/train_llm_maddpg_complete.py:230-246`
  - episodes、max_steps、train_frequency、llm_episode_interval、llm_distill_weight、exploration_episodes
- 退火策略输出与权重变更：`experiments/llm_maddpg/train_llm_maddpg_complete.py:269-287`
- 共享缓冲区大小（默认 100000）：`experiments/llm_maddpg/train_llm_maddpg_complete.py:212-214`

### 纯 LLM
- episodes 与步数来源：`experiments/llm/train_llm.py:56-58`（episodes 优先 `training.episodes`，步数用 `maddpg.max_steps`）
- LLM 服务参数读取：`llm_assistant/llm_client.py:28-38`

### MAPPO
- 训练超参：`utils/config.py:75-87` 默认；已在 YAML 增加可覆盖段 `config.yaml:162+`（mappo）
- 训练脚本读取与应用：`experiments/mappo/train_mappo.py:49-61`（max_episodes、max_steps、lr_*、gamma、lam、clip_range、entropy/value coeff、update_epochs、batch_size）

### HAPPO
- 训练超参：`utils/config.py:61-74` 默认；已在 YAML 增加可覆盖段 `config.yaml:162+`（happo）
- 训练脚本读取与应用：`experiments/happo/train_happo.py:49-62`（同 MAPPO，并额外 `kl_coeff`）

### Agent 内部超参使用
- MAPPO：`algos/mappo/mappo_agent.py:19-28`（lr、gamma、lam、clip_range、entropy/value coeff、update_epochs、batch_size）
- HAPPO：`algos/happo/happo_agent.py:19-28`（同上，外加 `kl_coeff`）

## 改进计划（不执行变更，征求确认）
1. 环境步数来源统一
   - 将 `CloudEdgeDeviceEnv` 的 `max_steps` 改为优先读取 `training.max_steps_per_episode`，如缺省则回退当前算法段（maddpg/mappo/happo/llm_maddpg）对应的 `max_steps`，避免对非 MADDPG 算法的截断不一致。
2. episodes 覆盖策略一致化
   - 在 MAPPO/HAPPO 训练脚本中也支持 `training.episodes` 覆盖，以统一 CLI/配置对所有算法的控制。
3. 配置校验与提示
   - 在加载时对关键字段进行类型与范围校验（如 lr>0、gamma∈(0,1)、batch_size≥1），遇到异常给出明确警告并使用安全默认值。
4. 文档补充
   - 在 `README.md` 增加“参数来源与覆盖顺序”章节，列出每算法读取路径与覆盖层级，减少误用风险。

请确认是否按以上计划统一参数来源与覆盖策略；确认后我将一次性完成上述改动并回归测试。