import numpy as np
import os
import random
import torch
import gymnasium as gym
from environment.cloud_edge_env import CloudEdgeDeviceEnv
from algos.maddpg_agent import MADDPGAgent
from algos.replay_buffer import ReplayBuffer
from utils.plotting import Plotter
from utils.metrics import MetricsTracker
from utils.config import load_config
from utils.path_manager import get_path_manager
from utils.csv_saver import save_training_metrics_csv

def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def train_maddpg(config=None):
    # åŠ è½½é…ç½®
    if config is None:
        config = load_config()
    set_seed(config.get('seed', 42))

    # ä½¿ç”¨è·¯å¾„ç®¡ç†å™¨
    path_manager = get_path_manager()
    
    # åˆ›å»ºä¿å­˜ç›®å½• - ä½¿ç”¨æ­£ç¡®çš„ç®—æ³•åç§°
    model_dir = path_manager.get_model_path("maddpg")
    data_dir = path_manager.get_data_path("csv")
    json_dir = path_manager.get_data_path("json")
    plot_dir = path_manager.get_plot_path()
    log_dir = path_manager.get_log_path()
    
    os.makedirs(model_dir, exist_ok=True)
    os.makedirs(data_dir, exist_ok=True)
    os.makedirs(json_dir, exist_ok=True)
    os.makedirs(plot_dir, exist_ok=True)
    os.makedirs(log_dir, exist_ok=True)

    # åˆ›å»ºç¯å¢ƒ
    env = CloudEdgeDeviceEnv(config['environment'])

    # åˆ›å»ºMADDPGæ™ºèƒ½ä½“
    # ä½¿ç”¨æ­£ç¡®çš„å•ä¸ªAgentçŠ¶æ€ç»´åº¦
    state_dim = env.get_agent_state_dim()  # 20ç»´ï¼š3(è‡ªå·±UE) + 10(æ‰€æœ‰ES) + 1(CS) + 6(è‡ªå·±ä»»åŠ¡)
    action_dim = env.action_space.shape[0]
    max_action = env.action_space.high[1] + 1  # ç›®æ ‡èŠ‚ç‚¹æ•°é‡
    num_agents = env.num_devices

    print(f"ğŸ”§ [MADDPG] Agenté…ç½®ä¿¡æ¯:")
    print(f"  å•ä¸ªAgentçŠ¶æ€ç»´åº¦: {state_dim}")
    print(f"  å…¨å±€çŠ¶æ€ç»´åº¦: {env.observation_space.shape[0]}")
    print(f"  åŠ¨ä½œç»´åº¦: {action_dim}")
    print(f"  è®¾å¤‡æ•°é‡: {num_agents}")
    print(f"  æ¨¡å‹ä¿å­˜è·¯å¾„: {model_dir}")

    # åˆ›å»ºå…±äº«ç»éªŒå›æ”¾ç¼“å†²åŒº
    buffer_size = config['maddpg']['buffer_size']
    shared_buffer = ReplayBuffer(buffer_size)
    print(f"  å…±äº«ç¼“å†²åŒºå¤§å°: {buffer_size}")

    agents = []
    for i in range(num_agents):
        agent = MADDPGAgent(
            state_dim=state_dim,
            action_dim=action_dim,
            max_action=max_action,
            num_agents=num_agents,
            agent_idx=i,
            lr_actor=config['maddpg']['lr_actor'],
            lr_critic=config['maddpg']['lr_critic'],
            gamma=config['maddpg']['gamma'],
            tau=config['maddpg']['tau'],
            buffer_size=config['maddpg']['buffer_size'],
            batch_size=config['maddpg']['batch_size']
        )
        agents.append(agent)

    # åˆ›å»ºç»˜å›¾å™¨å’ŒæŒ‡æ ‡è¿½è¸ªå™¨
    plotter = Plotter(plot_dir)
    metrics_tracker = MetricsTracker()

    # è®­ç»ƒå‚æ•°
    max_episodes = config['maddpg']['max_episodes']
    max_steps = config['maddpg']['max_steps']
    train_frequency = config['maddpg']['train_frequency']
    
    # è¯»å–è®­ç»ƒç­–ç•¥å‚æ•°
    save_frequency = config['training']['save_frequency']
    log_frequency = config['training']['log_frequency']
    warm_up_episodes = config['training']['warm_up_episodes']
    
    global_step_count = 0  # å…¨å±€æ­¥æ•°è®¡æ•°å™¨
    
    print(f"ğŸ”§ [MADDPG] è®­ç»ƒç­–ç•¥:")
    print(f"  è®­ç»ƒè½®æ•°: {max_episodes}")
    print(f"  æ¯è½®æœ€å¤§æ­¥æ•°: {max_steps}")
    print(f"  è®­ç»ƒé¢‘ç‡: æ¯{train_frequency}æ­¥è®­ç»ƒä¸€æ¬¡")
    print(f"  æ‰¹æ¬¡å¤§å°: {config['maddpg']['batch_size']}")
    print(f"  é¢„çƒ­è½®æ•°: {warm_up_episodes}")

    all_actions = []
    training_losses = []  # è®°å½•è®­ç»ƒæŸå¤±
    convergence_rewards = []  # æ”¶æ•›ç›‘æ§

    for episode in range(max_episodes):
        state, _ = env.reset()
        episode_reward = 0
        episode_latencies = []  # ğŸ†• æ”¶é›†æ¯æ­¥çš„å»¶è¿Ÿ
        episode_energies = []   # ğŸ†• æ”¶é›†æ¯æ­¥çš„èƒ½è€—
        
        # åˆ¤æ–­æ˜¯å¦åœ¨é¢„çƒ­é˜¶æ®µ
        is_warm_up = episode < warm_up_episodes
        
        for step in range(max_steps):
            global_step_count += 1
            
            # é€‰æ‹©åŠ¨ä½œï¼ˆä½¿ç”¨æ­£ç¡®çš„AgentçŠ¶æ€æå–ï¼‰
            actions = []
            for i, agent in enumerate(agents):
                agent_state = env.extract_agent_state(state, i)
                # ä¼˜åŒ–æ¢ç´¢ç­–ç•¥ï¼šé¢„çƒ­æœŸå’Œè®­ç»ƒæ—©æœŸå¢åŠ æ¢ç´¢
                if is_warm_up:
                    add_noise = True  # é¢„çƒ­æœŸå§‹ç»ˆæ¢ç´¢
                else:
                    add_noise = episode < max_episodes * 0.7  # è®­ç»ƒæœŸå‰70%æ¢ç´¢
                action = agent.select_action(agent_state, add_noise=add_noise, llm_advice=None)
                actions.append(action)
            
            # å°†actionsè½¬æ¢ä¸ºnumpyæ•°ç»„ä»¥æ»¡è¶³ç¯å¢ƒè¦æ±‚
            actions = np.array(actions)
            all_actions.append(actions)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, rewards, terminated, truncated, info = env.step(actions)
            done = terminated or truncated
            
            # ğŸ†• ä»infoä¸­æå–å»¶è¿Ÿå’Œèƒ½è€—ï¼Œè¿‡æ»¤é›¶å€¼
            if info:
                step_latencies = info.get('total_latencies', [])
                step_energies = info.get('total_energies', [])
                
                # è¿‡æ»¤æ‰é›¶å€¼ï¼Œåªä¿ç•™æœ‰æ•ˆçš„ä»»åŠ¡å¤„ç†æ•°æ®
                valid_latencies = [lat for lat in step_latencies if lat > 0]
                valid_energies = [eng for eng in step_energies if eng > 0]
                
                if valid_latencies:
                    episode_latencies.extend(valid_latencies)
                if valid_energies:
                    episode_energies.extend(valid_energies)
            
            # å­˜å‚¨ç»éªŒåˆ°å…±äº«ç¼“å†²åŒºï¼ˆæ‰€æœ‰Agentçš„ç»éªŒæ··åˆå­˜å‚¨ï¼‰
            for i in range(num_agents):
                agent_state = env.extract_agent_state(state, i)
                agent_next_state = env.extract_agent_state(next_state, i)
                
                # å­˜å‚¨åˆ°å…±äº«ç¼“å†²åŒºï¼Œä½¿ç”¨æ ‡å‡†5å…ƒç»„æ ¼å¼
                shared_buffer.add(
                    state=agent_state,
                    action=actions[i],
                    reward=rewards[i],
                    next_state=agent_next_state,
                    done=done,
                    llm_action=None  # çº¯MADDPGæ— LLMä¸“å®¶åŠ¨ä½œ
                )
            
            # è®­ç»ƒæ¡ä»¶ï¼šéé¢„çƒ­æœŸ + è¾¾åˆ°è®­ç»ƒé¢‘ç‡ + ç¼“å†²åŒºå……è¶³
            should_train = (not is_warm_up and 
                          global_step_count % train_frequency == 0 and 
                          len(shared_buffer) > config['maddpg']['batch_size'])
            
            if should_train:
                # ä»å…±äº«ç¼“å†²åŒºé‡‡æ ·ç»éªŒ
                states, actions_batch, rewards_batch, next_states, dones, _ = shared_buffer.sample(config['maddpg']['batch_size'])
                
                # æ‰€æœ‰Agentä½¿ç”¨ç›¸åŒçš„é‡‡æ ·ç»éªŒè¿›è¡Œè®­ç»ƒ
                step_losses = []
                for agent in agents:
                    agent_losses = agent.train(agents, shared_buffer)
                    step_losses.append(agent_losses)
                
                # è®°å½•å¹³å‡è®­ç»ƒæŸå¤±
                if step_losses and all(losses for losses in step_losses):
                    avg_critic_loss = np.mean([losses.get('critic_loss', 0) for losses in step_losses])
                    avg_actor_loss = np.mean([losses.get('actor_loss', 0) for losses in step_losses])
                    training_losses.append({
                        'step': global_step_count,
                        'episode': episode,
                        'critic_loss': avg_critic_loss,
                        'actor_loss': avg_actor_loss
                    })
                
                # ä¼˜åŒ–æ—¥å¿—è¾“å‡ºé¢‘ç‡
                if (global_step_count // train_frequency) % 20 == 0:  # æ¯1000æ­¥æ‰“å°ä¸€æ¬¡è®­ç»ƒä¿¡æ¯
                    print(f"  Step {global_step_count}: å…±äº«ç¼“å†²åŒºå¤§å°={len(shared_buffer)}, "
                          f"CriticæŸå¤±={avg_critic_loss:.4f}, ActoræŸå¤±={avg_actor_loss:.4f}")
            
            state = next_state
            episode_reward += sum(rewards)
            
            if done:
                break
        
        # Episode ç»“æŸï¼Œç»Ÿè®¡æŒ‡æ ‡
        # ä½¿ç”¨å®é™…ä»»åŠ¡å®Œæˆç‡è€Œä¸æ˜¯å›ºå®šå€¼
        completion_stats = env.get_task_completion_rate()
        episode_completion_rate = completion_stats.get('on_time_completion_rate', 0.0)
        
        # è®¡ç®—å¹³å‡å»¶è¿Ÿå’Œèƒ½è€—
        avg_latency = np.mean(episode_latencies) if episode_latencies else 0.0
        avg_energy = np.mean(episode_energies) if episode_energies else 0.0
        
        # ğŸ†• æ­£ç¡®ä¼ å…¥å»¶è¿Ÿå’Œèƒ½è€—æŒ‡æ ‡
        metrics_tracker.add_episode(episode_reward, avg_latency, avg_energy, False)
        convergence_rewards.append(episode_reward)
        
        # ä¼˜åŒ–è¿›åº¦æ‰“å°
        if (episode + 1) % log_frequency == 0:
            avg_metrics = metrics_tracker.get_average_metrics(last_n=log_frequency)
            status = "é¢„çƒ­é˜¶æ®µ" if is_warm_up else "è®­ç»ƒé˜¶æ®µ"
            print(f"[MADDPG] Episode {episode + 1}/{max_episodes} ({status})")
            print(f"  å¹³å‡å¥–åŠ±: {avg_metrics['avg_reward']:.2f}")
            print(f"  ç¼“å†²åŒºå¤§å°: {len(shared_buffer)}")
        
        # å®šæœŸä¿å­˜æ¨¡å‹
        if (episode + 1) % save_frequency == 0:
            for i, agent in enumerate(agents):
                # ä¿å­˜ä¸ºåˆ†ç¦»æ ¼å¼ (actorå’Œcriticåˆ†åˆ«ä¿å­˜)
                actor_path = path_manager.get_model_file_path("maddpg", f"actor_agent_{i}_episode_{episode+1}.pth")
                critic_path = path_manager.get_model_file_path("maddpg", f"critic_agent_{i}_episode_{episode+1}.pth")
                torch.save(agent.actor.state_dict(), actor_path)
                torch.save(agent.critic.state_dict(), critic_path)
            print(f"  âœ… æ¨¡å‹å·²ä¿å­˜è‡³ Episode {episode + 1}")

    # è®­ç»ƒå®Œæˆï¼Œä¿å­˜æœ€ç»ˆæ¨¡å‹
    print("\nğŸ¯ [MADDPG] è®­ç»ƒå®Œæˆï¼Œä¿å­˜æœ€ç»ˆæ¨¡å‹...")
    for i, agent in enumerate(agents):
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹ (åˆ†ç¦»æ ¼å¼)
        actor_path = path_manager.get_model_file_path("maddpg", f"actor_agent_{i}_final.pth")
        critic_path = path_manager.get_model_file_path("maddpg", f"critic_agent_{i}_final.pth")
        torch.save(agent.actor.state_dict(), actor_path)
        torch.save(agent.critic.state_dict(), critic_path)
        print(f"  âœ… Agent {i}: {actor_path}")
        print(f"  âœ… Agent {i}: {critic_path}")

    # ä¿å­˜è®­ç»ƒæŒ‡æ ‡åˆ°CSV
    print("\nğŸ“Š ä¿å­˜è®­ç»ƒæŒ‡æ ‡...")
    
    # è®¡ç®—æ‰€æœ‰episodeçš„å¹³å‡å®Œæˆç‡ä½œä¸ºä»£è¡¨å€¼
    # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œåœ¨å®é™…ç¯å¢ƒä¸­åº”è¯¥å•ç‹¬è®°å½•æ¯ä¸ªepisodeçš„å®Œæˆç‡
    final_completion_stats = env.get_task_completion_rate()
    avg_completion_rate = final_completion_stats.get('on_time_completion_rate', 0.0)
    
    try:
        csv_file = save_training_metrics_csv(
            episode_rewards=metrics_tracker.episode_rewards,
            episode_latencies=metrics_tracker.episode_delays,
            episode_energies=metrics_tracker.episode_energy,
            episode_completion_rates=[avg_completion_rate] * len(metrics_tracker.episode_rewards),
            algorithm_name="MADDPG",
            save_dir=data_dir
        )
        print(f"  âœ… CSVæ–‡ä»¶: {csv_file}")
    except Exception as e:
        print(f"  âŒ CSVä¿å­˜å¤±è´¥: {e}")

    # ä¿å­˜è¯¦ç»†è®­ç»ƒæ•°æ®åˆ°JSON
    try:
        training_data = {
            'algorithm': 'MADDPG',
            'config': config['maddpg'],
            'training_losses': training_losses,
            'convergence_rewards': convergence_rewards,
            'final_metrics': metrics_tracker.get_average_metrics(),
            'total_steps': global_step_count
        }
        json_file = path_manager.get_data_file_path("json", "maddpg_training_stats.json")
        with open(json_file, 'w', encoding='utf-8') as f:
            import json
            json.dump(training_data, f, indent=2, ensure_ascii=False, default=str)
        print(f"  âœ… JSONæ–‡ä»¶: {json_file}")
    except Exception as e:
        print(f"  âŒ JSONä¿å­˜å¤±è´¥: {e}")

    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    try:
        plotter.plot_rewards(metrics_tracker.episode_rewards)
        if all_actions:
            plotter.plot_action_distribution(np.array(all_actions).reshape(-1, action_dim))
        print(f"  âœ… å›¾è¡¨ä¿å­˜è‡³: {plot_dir}")
    except Exception as e:
        print(f"  âŒ å›¾è¡¨ä¿å­˜å¤±è´¥: {e}")

    print(f"\nğŸ‰ [MADDPG] è®­ç»ƒå®Œæˆ!")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {path_manager.get_experiment_dir()}")
    
    # è¿”å›è®­ç»ƒç»“æœ
    return {
        'episode_rewards': metrics_tracker.episode_rewards,
        'episode_latencies': metrics_tracker.episode_delays,
        'episode_energies': metrics_tracker.episode_energy,
        'episode_completion_rates': [avg_completion_rate] * len(metrics_tracker.episode_rewards),
        'training_losses': training_losses,
        'global_step_count': global_step_count,
        'algorithm': 'MADDPG',
        'model_paths': {
            'actor': [path_manager.get_model_file_path("maddpg", f"actor_agent_{i}_final.pth") for i in range(num_agents)],
            'critic': [path_manager.get_model_file_path("maddpg", f"critic_agent_{i}_final.pth") for i in range(num_agents)]
        }
    }

if __name__ == "__main__":
    train_maddpg()
