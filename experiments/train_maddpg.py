import numpy as np
import torch
import os
import random
import gymnasium as gym
from environment.cloud_edge_env import CloudEdgeDeviceEnv
from algos.maddpg_agent import MADDPGAgent
from utils.plotting import Plotter
from utils.metrics import MetricsTracker
from utils.config import load_config

def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def train_maddpg(config=None):
    # åŠ è½½é…ç½®
    if config is None:
        config = load_config()
    set_seed(config.get('seed', 42))

    # åˆ›å»ºä¿å­˜ç›®å½•
    save_dir = config.get('save_dir', 'results_maddpg')
    os.makedirs(save_dir, exist_ok=True)

    # åˆ›å»ºç¯å¢ƒ
    env = CloudEdgeDeviceEnv(config['environment'])

    # åˆ›å»ºMADDPGæ™ºèƒ½ä½“
    # ä½¿ç”¨æ­£ç¡®çš„å•ä¸ªAgentçŠ¶æ€ç»´åº¦
    state_dim = env.get_agent_state_dim()  # 20ç»´ï¼š3(è‡ªå·±UE) + 10(æ‰€æœ‰ES) + 1(CS) + 6(è‡ªå·±ä»»åŠ¡)
    action_dim = env.action_space.shape[0]
    max_action = env.action_space.high[1] + 1  # ç›®æ ‡èŠ‚ç‚¹æ•°é‡
    num_agents = env.num_devices

    print(f"ğŸ”§ [MADDPG] Agenté…ç½®ä¿¡æ¯:")
    print(f"  å•ä¸ªAgentçŠ¶æ€ç»´åº¦: {state_dim}")
    print(f"  å…¨å±€çŠ¶æ€ç»´åº¦: {env.observation_space.shape[0]}")
    print(f"  åŠ¨ä½œç»´åº¦: {action_dim}")
    print(f"  è®¾å¤‡æ•°é‡: {num_agents}")

    agents = []
    for i in range(num_agents):
        agent = MADDPGAgent(
            state_dim=state_dim,
            action_dim=action_dim,
            max_action=max_action,
            num_agents=num_agents,
            agent_idx=i,
            lr_actor=config['maddpg']['lr_actor'],
            lr_critic=config['maddpg']['lr_critic'],
            gamma=config['maddpg']['gamma'],
            tau=config['maddpg']['tau'],
            buffer_size=config['maddpg']['buffer_size'],
            batch_size=config['maddpg']['batch_size']
        )
        agents.append(agent)

    # åˆ›å»ºç»˜å›¾å™¨å’ŒæŒ‡æ ‡è¿½è¸ªå™¨
    plotter = Plotter(save_dir)
    metrics_tracker = MetricsTracker()

    # è®­ç»ƒå‚æ•°
    max_episodes = config['maddpg']['max_episodes']
    max_steps = config['maddpg']['max_steps']

    all_actions = []

    for episode in range(max_episodes):
        state, _ = env.reset()
        episode_reward = 0
        episode_delay = 0
        episode_energy = 0
        for step in range(max_steps):
            # é€‰æ‹©åŠ¨ä½œï¼ˆä½¿ç”¨æ­£ç¡®çš„AgentçŠ¶æ€æå–ï¼‰
            actions = []
            for i, agent in enumerate(agents):
                agent_state = env.extract_agent_state(state, i)
                action = agent.select_action(agent_state, llm_advice=None)
                actions.append(action)
            
            all_actions.append(actions)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, rewards, terminated, truncated, _ = env.step(actions)
            done = terminated or truncated
            
            # å­˜å‚¨ç»éªŒï¼ˆä½¿ç”¨æ­£ç¡®çš„çŠ¶æ€æå–ï¼‰
            for i, agent in enumerate(agents):
                agent_state = env.extract_agent_state(state, i)
                agent_next_state = env.extract_agent_state(next_state, i)
                agent.replay_buffer.add(agent_state, actions, rewards, agent_next_state, done)
            
            # è®­ç»ƒæ™ºèƒ½ä½“
            if len(agents[0].replay_buffer) > config['maddpg']['batch_size']:
                experiences = agents[0].replay_buffer.sample(config['maddpg']['batch_size'])
                for agent in agents:
                    agent.train(experiences, agents)
            state = next_state
            episode_reward += sum(rewards)
            # å¯é€‰ï¼šç»Ÿè®¡å»¶è¿Ÿã€èƒ½è€—ç­‰
            # episode_delay += ...
            # episode_energy += ...
            if done:
                break
        metrics_tracker.add_episode(episode_reward, episode_delay, episode_energy, None)
        if (episode + 1) % 10 == 0:
            avg_metrics = metrics_tracker.get_average_metrics(last_n=10)
            print(f"[MADDPG] Episode {episode + 1}/{max_episodes}")
            print(f"  å¹³å‡å¥–åŠ±: {avg_metrics['avg_reward']:.2f}")
        if (episode + 1) % 100 == 0:
            for i, agent in enumerate(agents):
                torch.save(agent.actor.state_dict(), f"{save_dir}/actor_agent_{i}_episode_{episode + 1}.pth")
                torch.save(agent.critic.state_dict(), f"{save_dir}/critic_agent_{i}_episode_{episode + 1}.pth")
            plotter.plot_rewards(metrics_tracker.episode_rewards)
            if all_actions:
                plotter.plot_action_distribution(np.array(all_actions).reshape(-1, 2))
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    for i, agent in enumerate(agents):
        torch.save(agent.actor.state_dict(), f"{save_dir}/actor_agent_{i}_final.pth")
        torch.save(agent.critic.state_dict(), f"{save_dir}/critic_agent_{i}_final.pth")
    plotter.plot_rewards(metrics_tracker.episode_rewards)
    if all_actions:
        plotter.plot_action_distribution(np.array(all_actions).reshape(-1, 2))
    print("[MADDPG] è®­ç»ƒå®Œæˆ!")
    return metrics_tracker.episode_rewards
