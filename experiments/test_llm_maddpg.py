# experiments/test_llm_maddpg.py
import numpy as np
import torch
import random
import os
import gymnasium as gym
from environment.cloud_edge_env import CloudEdgeDeviceEnv
from llm_assistant.llm_client import LLMClient
from llm_assistant.response_parser import ResponseParser
from algos.maddpg_agent import MADDPGAgent
from utils.config import load_config


def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def test_llm_maddpg(model_path, config=None):
    # åŠ è½½é…ç½®
    if config is None:
        config = load_config()
    set_seed(config.get('seed', 42))

    # åˆ›å»ºç¯å¢ƒ
    env = CloudEdgeDeviceEnv(config['environment'])

    # åˆ›å»ºLLMå®¢æˆ·ç«¯
    llm_client = LLMClient(
        api_key=config['llm'].get('api_key', ''),
        model_name=config['llm']['model_name'],
        server_url=config['llm'].get('server_url', 'http://10.200.1.35:8888/v1/completions'),
        timeout_connect=config['llm'].get('timeout_connect', 120),
        timeout_read=config['llm'].get('timeout_read', 300),
        use_mock=config['llm'].get('use_mock_when_unavailable', True)
    )

    # åˆ›å»ºMADDPGæ™ºèƒ½ä½“
    # ä½¿ç”¨æ­£ç¡®çš„å•ä¸ªAgentçŠ¶æ€ç»´åº¦  
    state_dim = env.get_agent_state_dim()  # 20ç»´ï¼š3(è‡ªå·±UE) + 10(æ‰€æœ‰ES) + 1(CS) + 6(è‡ªå·±ä»»åŠ¡)
    action_dim = env.action_space.shape[0]
    max_action = env.action_space.high[1] + 1  # ç›®æ ‡èŠ‚ç‚¹æ•°é‡
    num_agents = env.num_devices

    print(f"ğŸ”§ æµ‹è¯•Agenté…ç½®ä¿¡æ¯:")
    print(f"  å•ä¸ªAgentçŠ¶æ€ç»´åº¦: {state_dim}")
    print(f"  å…¨å±€çŠ¶æ€ç»´åº¦: {env.observation_space.shape[0]}")
    print(f"  åŠ¨ä½œç»´åº¦: {action_dim}")
    print(f"  è®¾å¤‡æ•°é‡: {num_agents}")

    agents = []
    for i in range(num_agents):
        agent = MADDPGAgent(
            state_dim=state_dim,
            action_dim=action_dim,
            max_action=max_action,
            num_agents=num_agents,
            agent_idx=i
        )

        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        actor_path = f"{model_path}/actor_agent_{i}_final.pth"
        critic_path = f"{model_path}/critic_agent_{i}_final.pth"

        if os.path.exists(actor_path):
            agent.actor.load_state_dict(torch.load(actor_path))
            agent.actor.eval()

        if os.path.exists(critic_path):
            agent.critic.load_state_dict(torch.load(critic_path))
            agent.critic.eval()

        agents.append(agent)

    # è®¾å¤‡ã€è¾¹ç¼˜å’Œäº‘ç«¯è¯¦ç»†ä¿¡æ¯
    device_info = [{"cpu": device.cpu_frequency} for device in env.devices]
    edge_info = [{"cpu": edge.cpu_frequency} for edge in env.edge_servers]
    cloud_info = [{"cpu": cloud.cpu_frequency} for cloud in env.cloud_servers]

    # æµ‹è¯•å‚æ•°
    num_episodes = config['testing']['num_episodes']
    max_steps = config['maddpg']['max_steps']

    # è®°å½•æ‰€æœ‰episodeçš„æŒ‡æ ‡
    all_episode_energy = []
    all_episode_delay = []
    all_episode_util = []

    for episode in range(num_episodes):
        state, _ = env.reset()
        episode_reward = 0
        episode_energy = 0
        episode_delay = 0
        episode_util = 0
        step_count = 0

        print(f"\næµ‹è¯• Episode {episode + 1}/{num_episodes}")

        for step in range(max_steps):
            # é€‰æ‹©åŠ¨ä½œ
            actions = []
            
            # æ¯æ­¥éƒ½å’¨è¯¢LLMè·å–å»ºè®®
            strategies = llm_client.get_unload_strategy(state, device_info, edge_info, cloud_info)
            llm_advice = ResponseParser.parse_unload_strategy(
                strategies,
                env.num_devices,
                env.num_edges,
                env.num_clouds
            )
            
            for i, agent in enumerate(agents):
                # ä½¿ç”¨æ­£ç¡®çš„AgentçŠ¶æ€æå–
                agent_state = env.extract_agent_state(state, i)
                
                # ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“æä¾›LLMæŒ‡å¯¼
                if llm_advice:
                    agent_llm_advice = next((a for a in llm_advice if a["task_id"] == i), None)
                    if agent_llm_advice:
                        # æ„å»ºåˆé€‚çš„LLMå»ºè®®å¼ é‡
                        advice_tensor = torch.tensor([
                            [
                                agent_llm_advice.get("offload_ratio", 0.0),  # å¸è½½æ¯”ä¾‹
                                agent_llm_advice.get("target_node", 0.0)     # ç›®æ ‡èŠ‚ç‚¹
                            ]
                        ], dtype=torch.float32)
                        agent_action = agent.select_action(agent_state, add_noise=False, llm_advice=advice_tensor)
                    else:
                        # å¦‚æœæ²¡æœ‰é’ˆå¯¹è¯¥æ™ºèƒ½ä½“çš„å»ºè®®
                        agent_action = agent.select_action(agent_state, add_noise=False, llm_advice=None)
                else:
                    # æ²¡æœ‰LLMå»ºè®®
                    agent_action = agent.select_action(agent_state, add_noise=False, llm_advice=None)
                
                actions.append(agent_action)

            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, rewards, terminated, truncated, info = env.step(actions)
            done = terminated or truncated

            # æ‰“å°è¯¦ç»†ä¿¡æ¯
            if step % 10 == 0:
                print(f"æ­¥éª¤ {step}:")
                for i, (action, reward) in enumerate(zip(actions, rewards)):
                    offload_ratio = action[0]
                    target_node = int(action[1])

                    target_name = "æœ¬åœ°"
                    if target_node >= 1 and target_node <= env.num_edges:
                        target_name = f"è¾¹ç¼˜æœåŠ¡å™¨ {target_node - 1}"
                    elif target_node > env.num_edges:
                        target_name = "äº‘ç«¯"

                    print(f"  è®¾å¤‡ {i}: å¸è½½æ¯”ä¾‹={offload_ratio:.2f}, ç›®æ ‡={target_name}, å¥–åŠ±={reward:.2f}")

            # ç´¯åŠ èƒ½è€—ã€æ—¶å»¶ã€èµ„æºåˆ©ç”¨ç‡
            episode_energy += sum(info['energies'])
            episode_delay += sum(info['delays'])
            episode_util += sum(info['utilizations'])
            step_count += 1

            state = next_state
            episode_reward += sum(rewards)

            if done:
                print(f"Episode {episode + 1} å®Œæˆï¼Œæ€»å¥–åŠ±: {episode_reward:.2f}")
                break

        avg_energy = episode_energy / num_agents
        avg_delay = episode_delay / num_agents
        avg_util = episode_util / (num_agents * step_count) if step_count > 0 else 0
        all_episode_energy.append(avg_energy)
        all_episode_delay.append(avg_delay)
        all_episode_util.append(avg_util)
        print(f"Episode {episode + 1} å¹³å‡èƒ½è€—: {avg_energy:.4f}, å¹³å‡èµ„æºåˆ©ç”¨ç‡: {avg_util:.4f}, å¹³å‡ä»»åŠ¡æ—¶å»¶: {avg_delay:.4f}")

    print("æµ‹è¯•å®Œæˆ!")
    return all_episode_energy, all_episode_util, all_episode_delay