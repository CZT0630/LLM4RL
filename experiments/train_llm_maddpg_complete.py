# experiments/train_llm_maddpg_complete.py
"""
å®Œæ•´ç‰ˆLLM+MADDPGè®­ç»ƒè„šæœ¬
å®ç°ï¼šæ¯stepæ–°ä»»åŠ¡ -> LLMå’¨è¯¢ï¼ˆäº¤æ›¿ï¼‰ -> AgentåŠ¨ä½œ -> æ‰§è¡Œ -> æ¯20æ­¥è®­ç»ƒä¸€æ¬¡
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import numpy as np
import torch
import yaml
import json
from tqdm import tqdm
import logging
from datetime import datetime
import matplotlib.pyplot as plt
import random

from environment.cloud_edge_env import CloudEdgeDeviceEnv
from algos.maddpg_agent import MADDPGAgent
from algos.replay_buffer import ReplayBuffer
from llm_assistant.llm_client import LLMClient
from llm_assistant.prompt_builder import PromptBuilder
from llm_assistant.response_parser import ResponseParser
from utils.config import load_config
from utils.path_manager import get_path_manager
from utils.csv_saver import save_training_metrics_csv
from utils.metrics import MetricsTracker
# from utils.plotting import plot_training_curves
# from utils.metrics import calculate_episode_metrics


def setup_logging(path_manager):
    """è®¾ç½®æ—¥å¿—"""
    log_dir = path_manager.get_log_path()
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = path_manager.get_log_file_path(f"train_llm_maddpg_{timestamp}.log")
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(__name__)


def create_agents(env, config):
    """åˆ›å»ºæ‰€æœ‰Agent"""
    agents = []
    # ä½¿ç”¨æ­£ç¡®çš„å•ä¸ªAgentçŠ¶æ€ç»´åº¦
    state_dim = env.get_agent_state_dim()  # 20ç»´ï¼š3(è‡ªå·±UE) + 10(æ‰€æœ‰ES) + 1(CS) + 6(è‡ªå·±ä»»åŠ¡)
    action_dim = 4  # [Î±1, Î±2, Î±3, edge_id]
    
    print(f"ğŸ”§ Agenté…ç½®ä¿¡æ¯:")
    print(f"  å•ä¸ªAgentçŠ¶æ€ç»´åº¦: {state_dim}")
    print(f"  å…¨å±€çŠ¶æ€ç»´åº¦: {env.observation_space.shape[0]}")
    print(f"  åŠ¨ä½œç»´åº¦: {action_dim}")
    print(f"  è®¾å¤‡æ•°é‡: {env.num_devices}")
    
    # ğŸ†• è¯»å–é€€ç«ç­–ç•¥é…ç½®
    llm_config = config.get('llm_maddpg', {})
    use_annealing = llm_config.get('use_annealing', False)
    
    if use_annealing:
        print(f"ğŸ”¥ é€€ç«ç­–ç•¥é…ç½®:")
        print(f"  å¯ç”¨çŠ¶æ€: {use_annealing}")
        print(f"  åˆå§‹æƒé‡: {llm_config.get('initial_llm_distill_weight', 0.8)}")
        print(f"  æ’å®šæƒé‡: {llm_config.get('constant_llm_distill_weight', 0.15)}")
        print(f"  æœ€ç»ˆæƒé‡: {llm_config.get('final_llm_distill_weight', 0.0)}")
        print(f"  é˜¶æ®µ1ç»“æŸ: {llm_config.get('stage1_end_episode', 300)} episodes")
        print(f"  é˜¶æ®µ2ç»“æŸ: {llm_config.get('stage2_end_episode', 700)} episodes")
    else:
        print("â„¹ï¸  é€€ç«ç­–ç•¥æœªå¯ç”¨ï¼Œä½¿ç”¨å›ºå®šè’¸é¦æƒé‡")
    
    for i in range(env.num_devices):
        # æ„å»ºAgenté…ç½®ï¼ŒåŒ…å«é€€ç«ç­–ç•¥å’Œè®­ç»ƒå‚æ•°
        agent_config = config['training'].copy()
        agent_config.update(llm_config)  # æ·»åŠ LLMé…ç½®åŒ…æ‹¬é€€ç«ç­–ç•¥
        
        agent = MADDPGAgent(
            state_dim=state_dim,
            action_dim=action_dim,
            num_agents=env.num_devices,
            agent_idx=i,
            config=agent_config
        )
        agents.append(agent)
        
    return agents


def consult_llm_for_all_devices(env, llm_client, prompt_builder, response_parser, logger):
    """ä¸ºæ‰€æœ‰è®¾å¤‡å’¨è¯¢LLMè·å–ä¸“å®¶åŠ¨ä½œ"""
    try:
        # è·å–ç³»ç»ŸçŠ¶æ€ä¿¡æ¯
        device_info = env.get_device_info()
        edge_info = env.get_edge_info()
        cloud_info = env.get_cloud_info()
        task_info = env.get_current_tasks_info()
        
        # æ„å»ºLLMæç¤º
        prompt = prompt_builder.build_offloading_strategy_prompt(
            env_state=None,  # ç¯å¢ƒçŠ¶æ€ä¿¡æ¯
            device_info=device_info,
            edge_info=edge_info,
            cloud_info=cloud_info,
            tasks_info=task_info
        )
        
        # å’¨è¯¢LLM
        response = llm_client.query(prompt)
        
        # è§£æLLMå“åº”
        llm_actions = response_parser.parse_unload_strategy(response, env.num_devices, env.num_edges, env.num_clouds)
        
        # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼ [Î±1, Î±2, Î±3, edge_id]
        formatted_actions = []
        for i in range(env.num_devices):
            if i < len(llm_actions):
                action = llm_actions[i]
                # å°†ä¸‰å…ƒåˆ†å‰²å†³ç­–è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                alpha1 = action.get('local_ratio', 0.0)
                alpha2 = action.get('edge_ratio', 0.0)
                alpha3 = action.get('cloud_ratio', 1.0)
                edge_id = action.get('edge_server_id', 0)
                
                formatted_actions.append([alpha1, alpha2, alpha3, edge_id])
            else:
                # é»˜è®¤åŠ¨ä½œï¼šå…¨éƒ¨åœ¨äº‘ç«¯æ‰§è¡Œ
                formatted_actions.append([0.0, 0.0, 1, 0])
                
        return formatted_actions
        
    except Exception as e:
        logger.warning(f"LLMå’¨è¯¢å¤±è´¥: {e}")
        # è¿”å›é»˜è®¤åŠ¨ä½œ
        default_actions = [[0, 0, 1, 0] for _ in range(env.num_devices)]
        return default_actions


def train_agents_from_buffer(agents, shared_buffer, logger, step_count):
    """ä»å…±äº«ç¼“å†²åŒºè®­ç»ƒæ‰€æœ‰Agent"""
    if len(shared_buffer) < 64:  # æœ€å°æ‰¹é‡è¦æ±‚
        logger.debug(f"ç¼“å†²åŒºæ ·æœ¬ä¸è¶³ï¼Œè·³è¿‡è®­ç»ƒ (å½“å‰: {len(shared_buffer)})")
        return {}
    
    training_stats = {}
    
    for i, agent in enumerate(agents):
        try:
            stats = agent.train(agents, shared_buffer)
            training_stats[f'agent_{i}'] = stats
            
            if stats:
                logger.debug(f"Agent{i} è®­ç»ƒå®Œæˆ: "
                           f"critic_loss={stats.get('critic_loss', 0):.4f}, "
                           f"actor_loss={stats.get('actor_loss', 0):.4f}, "
                           f"distill_loss={stats.get('distill_loss', 0):.4f}")
                           
        except Exception as e:
            logger.error(f"Agent{i} è®­ç»ƒå¤±è´¥: {e}")
            
    logger.info(f"Step {step_count}: å®Œæˆæ‰€æœ‰Agentè®­ç»ƒ")
    return training_stats


def train_llm_maddpg_complete(config_path):
    """ä¸»è®­ç»ƒå‡½æ•° - å®Œæ•´çš„è®­ç»ƒæµç¨‹"""
    # åŠ è½½é…ç½®
    config = load_config(config_path)
    
    # ä½¿ç”¨è·¯å¾„ç®¡ç†å™¨
    path_manager = get_path_manager()
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_logging(path_manager)
    logger.info("å¼€å§‹å®Œæ•´ç‰ˆLLM+MADDPGè®­ç»ƒ")
    
    # åˆ›å»ºä¿å­˜ç›®å½• - ä½¿ç”¨æ­£ç¡®çš„ç®—æ³•åç§°
    model_dir = path_manager.get_model_path("llm_maddpg")
    data_dir = path_manager.get_data_path("csv")
    json_dir = path_manager.get_data_path("json")
    plot_dir = path_manager.get_plot_path()
    log_dir = path_manager.get_log_path()
    
    os.makedirs(model_dir, exist_ok=True)
    os.makedirs(data_dir, exist_ok=True)
    os.makedirs(json_dir, exist_ok=True)
    os.makedirs(plot_dir, exist_ok=True)
    os.makedirs(log_dir, exist_ok=True)
    
    logger.info(f"ğŸ”§ [LLM+MADDPG] è·¯å¾„é…ç½®:")
    logger.info(f"  æ¨¡å‹ä¿å­˜è·¯å¾„: {model_dir}")
    logger.info(f"  æ•°æ®ä¿å­˜è·¯å¾„: {data_dir}")
    logger.info(f"  å®éªŒç›®å½•: {path_manager.get_experiment_dir()}")
    
    # åˆ›å»ºç¯å¢ƒ
    logger.info("åˆ›å»ºäº‘è¾¹ç«¯ä¸‰å±‚æ¶æ„ç¯å¢ƒ...")
    env = CloudEdgeDeviceEnv(config)
    
    # åˆ›å»ºAgent
    logger.info("åˆ›å»ºMADDPGæ™ºèƒ½ä½“...")
    agents = create_agents(env, config)
    
    # åˆ›å»ºå…±äº«ç»éªŒå›æ”¾ç¼“å†²åŒº
    buffer_size = config['training'].get('buffer_size', 100000)
    shared_buffer = ReplayBuffer(buffer_size)
    
    # åˆ›å»ºLLMç»„ä»¶
    logger.info("åˆå§‹åŒ–LLMåŠ©æ‰‹...")
    try:
        llm_client = LLMClient(
            config=config,
            use_mock=config['llm'].get('use_mock_when_unavailable', False)
        )
        prompt_builder = PromptBuilder()
        response_parser = ResponseParser()
        llm_available = True
    except Exception as e:
        logger.warning(f"LLMåˆå§‹åŒ–å¤±è´¥ï¼Œå°†è·³è¿‡LLMå’¨è¯¢: {e}")
        llm_available = False
    
    # ğŸ†• ä»é…ç½®æ–‡ä»¶è¯»å–è®­ç»ƒå‚æ•°
    llm_config = config.get('llm_maddpg', {})
    
    # è®­ç»ƒå‚æ•°
    num_episodes = llm_config.get('max_episodes', config['training'].get('episodes', 1000))
    max_steps_per_episode = llm_config.get('max_steps', config['training'].get('max_steps_per_episode', 200))
    
    # ğŸ†• è®­ç»ƒç­–ç•¥å‚æ•°ï¼ˆä»é…ç½®æ–‡ä»¶è¯»å–ï¼‰
    train_frequency = llm_config.get('train_frequency', 50)  # æ¯12æ­¥è®­ç»ƒä¸€æ¬¡
    llm_episode_interval = llm_config.get('llm_episode_interval', 2)  # æ¯2ä¸ªEpisodeä½¿ç”¨ä¸€æ¬¡LLM
    llm_distill_weight = llm_config.get('llm_distill_weight', 0.1)  # LLMçŸ¥è¯†è’¸é¦æƒé‡
    exploration_episodes = llm_config.get('exploration_episodes', int(num_episodes * 0.9))  # æ¢ç´¢è½®æ•°
    
    # ğŸ†• è¯»å–è®­ç»ƒç­–ç•¥å‚æ•°
    save_frequency = config['training']['save_frequency']
    log_frequency = config['training']['log_frequency']
    warm_up_episodes = config['training']['warm_up_episodes']
    
    # è®°å½•æŒ‡æ ‡ - ä½¿ç”¨MetricsTrackerç±»ä¿æŒä¸å…¶ä»–ç®—æ³•ä¸€è‡´
    metrics_tracker = MetricsTracker()
    training_losses = []
    
    # å…¨å±€stepè®¡æ•°å™¨
    global_step_count = 0
    
    logger.info(f"ğŸ”§ [LLM+MADDPG] è®­ç»ƒç­–ç•¥é…ç½®:")
    logger.info(f"  è®­ç»ƒè½®æ•°: {num_episodes}")
    logger.info(f"  æ¯è½®æœ€å¤§æ­¥æ•°: {max_steps_per_episode}")
    logger.info(f"  è®­ç»ƒé¢‘ç‡: æ¯{train_frequency}æ­¥è®­ç»ƒä¸€æ¬¡")
    logger.info(f"  LLMæŒ‡å¯¼é—´éš”: æ¯{llm_episode_interval}ä¸ªEpisodeä½¿ç”¨LLM")
    logger.info(f"  é¢„çƒ­è½®æ•°: {warm_up_episodes}")
    logger.info(f"  æ¢ç´¢è½®æ•°: {exploration_episodes}")
    logger.info(f"  çŸ¥è¯†è’¸é¦æƒé‡: {llm_distill_weight}")
    
    for episode in tqdm(range(num_episodes), desc="è®­ç»ƒè¿›åº¦"):
        logger.info(f"\n{'='*80}")
        logger.info(f"Episode {episode + 1}/{num_episodes}")
        logger.info(f"{'='*80}")
        
        # ğŸ†• æ›´æ–°æ‰€æœ‰Agentçš„LLMè’¸é¦æƒé‡ï¼ˆé€€ç«ç­–ç•¥ï¼‰
        for agent in agents:
            if hasattr(agent, 'update_llm_distill_weight'):
                old_weight = agent.llm_distill_weight
                new_weight = agent.update_llm_distill_weight(episode)
                
                # æ˜¾ç¤ºæƒé‡å˜åŒ–ï¼ˆä»…åœ¨å˜åŒ–æ—¶æ˜¾ç¤ºï¼‰
                if abs(old_weight - new_weight) > 0.001:
                    stage_name, stage_desc = agent.get_current_annealing_stage(episode)
                    logger.info(f"ğŸ”¥ é€€ç«ç­–ç•¥æ›´æ–°: {stage_name}")
                    logger.info(f"    {stage_desc}")
                    logger.info(f"    æƒé‡å˜åŒ–: {old_weight:.3f} â†’ {new_weight:.3f}")
        
        # ğŸ†• æ˜¾ç¤ºå½“å‰é€€ç«é˜¶æ®µï¼ˆç¬¬ä¸€ä¸ªAgentçš„çŠ¶æ€ä»£è¡¨æ‰€æœ‰Agentï¼‰
        if hasattr(agents[0], 'get_current_annealing_stage'):
            stage_name, stage_desc = agents[0].get_current_annealing_stage(episode)
            current_weight = agents[0].llm_distill_weight
            logger.info(f"ğŸ“Š å½“å‰è’¸é¦çŠ¶æ€: {stage_desc} (å½“å‰æƒé‡: {current_weight:.3f})")
        
        # ğŸ†• åˆ¤æ–­å½“å‰è®­ç»ƒé˜¶æ®µ
        is_warm_up = episode < warm_up_episodes
        is_exploration = episode < exploration_episodes
        
        # åˆ¤æ–­æ˜¯å¦åœ¨å½“å‰Episodeä½¿ç”¨LLM
        use_llm_this_episode = (episode % llm_episode_interval == 0) and llm_available and not is_warm_up
        
        stage = "é¢„çƒ­é˜¶æ®µ" if is_warm_up else ("æ¢ç´¢é˜¶æ®µ" if is_exploration else "æ”¶æ•›é˜¶æ®µ")
        llm_status = "ä½¿ç”¨LLMæŒ‡å¯¼" if use_llm_this_episode else "çº¯MADDPGè®­ç»ƒ"
        logger.info(f"Episode {episode + 1}: {stage} - {llm_status}")
        
        # é‡ç½®ç¯å¢ƒ
        state, _ = env.reset()
        step_means = []  # æ–°å¢ï¼šæ”¶é›†æ¯ä¸ªstepæ‰€æœ‰æ™ºèƒ½ä½“rewardçš„å‡å€¼
        episode_latencies = []  # ğŸ†• æ”¶é›†æ¯æ­¥çš„å»¶è¿Ÿ
        episode_energies = []   # ğŸ†• æ”¶é›†æ¯æ­¥çš„èƒ½è€—
        
        # Episodeå¾ªç¯
        for step in range(max_steps_per_episode):
            global_step_count += 1
            
            logger.debug(f"\nEpisode {episode + 1}, Step {step + 1}")

            # å½“å‰stepçš„çŠ¶æ€
            current_state = state.copy()
            
            # 1. LLMå’¨è¯¢ï¼ˆå¦‚æœå½“å‰Episodeä½¿ç”¨LLMï¼‰
            llm_expert_actions = None
            if use_llm_this_episode:
                llm_expert_actions = consult_llm_for_all_devices(
                    env, llm_client, prompt_builder, response_parser, logger
                )
                logger.debug(f"è·å–LLMä¸“å®¶åŠ¨ä½œ: {len(llm_expert_actions)}ä¸ªè®¾å¤‡")
            
            # 2. AgentåŠ¨ä½œç”Ÿæˆ
            agent_actions = []
            print(f"\nğŸ“‹ MADDPGæ™ºèƒ½ä½“ç­–ç•¥ç”Ÿæˆ:")
            print(f"{'='*80}")
            
            for i, agent in enumerate(agents):
                # ä½¿ç”¨æ­£ç¡®çš„AgentçŠ¶æ€æå–æ–¹æ³•
                agent_state = env.extract_agent_state(current_state, i)
                
                # ğŸ†• ä¼˜åŒ–æ¢ç´¢ç­–ç•¥
                if is_warm_up:
                    add_noise = True  # é¢„çƒ­æœŸå§‹ç»ˆæ¢ç´¢
                elif is_exploration:
                    add_noise = True  # æ¢ç´¢æœŸå§‹ç»ˆæ¢ç´¢
                else:
                    add_noise = episode < num_episodes * 0.9  # æ”¶æ•›æœŸé€‚åº¦æ¢ç´¢
                    
                action = agent.select_action(agent_state, add_noise=add_noise)
                agent_actions.append(action)
                
                # è¯¦ç»†è¾“å‡ºMADDPGç­–ç•¥
                alpha1, alpha2, alpha3 = action[:3]
                edge_id = int(action[-1]) if len(action) >= 4 else 0
                
                # å½’ä¸€åŒ–åˆ†å‰²æ¯”ä¾‹
                total = alpha1 + alpha2 + alpha3
                if total > 0:
                    alpha1_norm, alpha2_norm, alpha3_norm = alpha1/total, alpha2/total, alpha3/total
                else:
                    alpha1_norm, alpha2_norm, alpha3_norm = 1.0, 0.0, 0.0
                
                print(f"  ğŸ¤– Agent{i} (Device{i}) MADDPGç­–ç•¥:")
                print(f"    AgentçŠ¶æ€ç»´åº¦: {len(agent_state)} (æ­£ç¡®æå–)")
                print(f"    åŸå§‹åŠ¨ä½œ: [Î±1={alpha1:.3f}, Î±2={alpha2:.3f}, Î±3={alpha3:.3f}, edge={action[-1]:.3f}]")
                print(f"    å½’ä¸€åŒ–åˆ†å‰²: [æœ¬åœ°:{alpha1_norm:.3f}, è¾¹ç¼˜:{alpha2_norm:.3f}, äº‘ç«¯:{alpha3_norm:.3f}]")
                print(f"    ç›®æ ‡è¾¹ç¼˜æœåŠ¡å™¨: Edge{edge_id}")
                print(f"    æ¢ç´¢æ¨¡å¼: {'å¼€å¯' if add_noise else 'å…³é—­'}")

            agent_actions = np.array(agent_actions)
            
            # 3. æ‰§è¡ŒåŠ¨ä½œã€ç¯å¢ƒäº¤äº’
            next_state, rewards, terminated, truncated, info = env.step(
                agent_actions, llm_actions=llm_expert_actions
            )
            
            # 4. å­˜å‚¨ç»éªŒåˆ°å…±äº«ç¼“å†²åŒº
            for i in range(env.num_devices):
                # ä½¿ç”¨æ­£ç¡®çš„AgentçŠ¶æ€æå–æ–¹æ³•
                agent_state = env.extract_agent_state(current_state, i)
                agent_next_state = env.extract_agent_state(next_state, i)
                
                shared_buffer.add(
                    state=agent_state,
                    action=agent_actions[i],
                    reward=rewards[i],
                    next_state=agent_next_state,
                    done=terminated or truncated,
                    llm_action=llm_expert_actions if use_llm_this_episode else None
                )
            
            # ğŸ†• è®­ç»ƒæ¡ä»¶ï¼šéé¢„çƒ­æœŸ + è¾¾åˆ°è®­ç»ƒé¢‘ç‡ + ç¼“å†²åŒºå……è¶³
            should_train = (not is_warm_up and 
                          global_step_count % train_frequency == 0 and
                          len(shared_buffer) > config['maddpg']['batch_size'])
            
            if should_train:
                logger.info(f"\n--- ç¬¬{global_step_count}æ­¥: å¼€å§‹è®­ç»ƒæ‰€æœ‰Agent ---")
                train_stats = train_agents_from_buffer(agents, shared_buffer, logger, global_step_count)
                training_losses.append(train_stats)
            
            # æ›´æ–°çŠ¶æ€å’Œå¥–åŠ±
            state = next_state
            # è®°å½•æœ¬stepæ‰€æœ‰æ™ºèƒ½ä½“rewardçš„å‡å€¼
            step_means.append(np.mean(rewards))
            
            # ğŸ†• ä»infoä¸­æå–å»¶è¿Ÿå’Œèƒ½è€—ï¼Œè¿‡æ»¤é›¶å€¼
            if info:
                step_latencies = info.get('total_latencies', [])
                step_energies = info.get('total_energies', [])
                
                # è¿‡æ»¤æ‰é›¶å€¼ï¼Œåªä¿ç•™æœ‰æ•ˆçš„ä»»åŠ¡å¤„ç†æ•°æ®
                valid_latencies = [lat for lat in step_latencies if lat > 0]
                valid_energies = [eng for eng in step_energies if eng > 0]
                
                if valid_latencies:
                    episode_latencies.extend(valid_latencies)
                if valid_energies:
                    episode_energies.extend(valid_energies)
            
            # æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶
            if terminated or truncated:
                logger.info(f"Episode {episode + 1} åœ¨ç¬¬{step + 1}æ­¥ç»ˆæ­¢")
                break
        
        # ç»Ÿä¸€episode rewardè®¡ç®—æ–¹å¼
        episode_reward = np.mean(step_means) if step_means else 0.0
        
        # Episodeç»“æŸï¼Œè®°å½•æŒ‡æ ‡ - ä½¿ç”¨MetricsTrackerä¿æŒä¸å…¶ä»–ç®—æ³•ä¸€è‡´
        # ä½¿ç”¨å®é™…ä»»åŠ¡å®Œæˆç‡è€Œä¸æ˜¯å›ºå®šå€¼
        completion_stats = env.get_task_completion_rate()
        episode_completion_rate = completion_stats.get('on_time_completion_rate', 0.0)
        
        # è®¡ç®—å¹³å‡å»¶è¿Ÿå’Œèƒ½è€—
        avg_latency = np.mean(episode_latencies) if episode_latencies else 0.0
        avg_energy = np.mean(episode_energies) if episode_energies else 0.0
        
        # ä½¿ç”¨MetricsTrackerè®°å½•episodeæŒ‡æ ‡
        metrics_tracker.add_episode(episode_reward, avg_latency, avg_energy, use_llm_this_episode)
        
        # ğŸ†• ä¼˜åŒ–è¿›åº¦æ‰“å° - ä½¿ç”¨MetricsTrackerè·å–ç»Ÿè®¡æ•°æ®
        if (episode + 1) % log_frequency == 0:
            avg_metrics = metrics_tracker.get_average_metrics(last_n=log_frequency)
            
            logger.info(f"\nEpisode {episode + 1} é˜¶æ®µæ€§æ€»ç»“ ({stage}):")
            logger.info(f"  æœ€è¿‘{log_frequency}è½®å¹³å‡å¥–åŠ±: {avg_metrics['avg_reward']:.3f}")
            logger.info(f"  æœ€è¿‘{log_frequency}è½®å¹³å‡æ—¶å»¶: {avg_metrics['avg_delay']:.3f}s")
            logger.info(f"  æœ€è¿‘{log_frequency}è½®å¹³å‡èƒ½è€—: {avg_metrics['avg_energy']:.3f}J")
            logger.info(f"  LLMä½¿ç”¨æ¯”ä¾‹: {avg_metrics['llm_usage_ratio']:.3f}")
            
            # æ˜¾ç¤ºè¯¦ç»†çš„ä»»åŠ¡å®Œæˆç»Ÿè®¡
            if info and 'task_completion_stats' in info:
                comp_stats = info['task_completion_stats']
                logger.info(f"  è¯¦ç»†å®Œæˆç‡ç»Ÿè®¡:")
                logger.info(f"    æ€»å®Œæˆç‡: {comp_stats.get('overall_completion_rate', 0):.3f}")
                logger.info(f"    æŒ‰æ—¶å®Œæˆç‡: {comp_stats.get('on_time_completion_rate', 0):.3f}")
                logger.info(f"    è¶…æ—¶å®Œæˆç‡: {comp_stats.get('timeout_rate', 0):.3f}")
                logger.info(f"    å¤±è´¥ç‡: {comp_stats.get('failure_rate', 0):.3f}")
                logger.info(f"    å¹³å‡è¶…æ—¶æ—¶é—´: {comp_stats.get('avg_overtime', 0):.2f}s")
            
            logger.info(f"  å…¨å±€æ­¥æ•°: {global_step_count}")
            logger.info(f"  ç¼“å†²åŒºå¤§å°: {len(shared_buffer)}")
            
            # ğŸ†• æ”¶æ•›æ£€æµ‹ - ä½¿ç”¨MetricsTrackeræ•°æ®
            if not is_warm_up and len(metrics_tracker.episode_rewards) >= 50:
                recent_rewards = metrics_tracker.episode_rewards[-50:]
                reward_std = np.std(recent_rewards)
                convergence_threshold = config['training']['convergence_threshold']
                if reward_std < convergence_threshold:
                    logger.info(f"  ğŸ¯ æ£€æµ‹åˆ°æ”¶æ•›ï¼å¥–åŠ±æ ‡å‡†å·®: {reward_std:.4f} < {convergence_threshold}")
        
        # ğŸ†• å®šæœŸä¿å­˜æ¨¡å‹
        if (episode + 1) % save_frequency == 0:
            for i, agent in enumerate(agents):
                model_path = path_manager.get_model_file_path("llm_maddpg", f"agent_{i}_episode_{episode + 1}.pth")
                agent.save_model(model_path)
            logger.info(f"Episode {episode + 1}: æ¨¡å‹å·²ä¿å­˜åˆ° {model_dir}")
    
    # è®­ç»ƒç»“æŸï¼Œä¿å­˜æœ€ç»ˆæ¨¡å‹
    logger.info("è®­ç»ƒå®Œæˆï¼Œä¿å­˜æœ€ç»ˆæ¨¡å‹...")
    for i, agent in enumerate(agents):
        final_model_path = path_manager.get_model_file_path("llm_maddpg", f"agent_{i}_final.pth")
        agent.save_model(final_model_path)
    
    # ä¿å­˜è®­ç»ƒç»Ÿè®¡æ•°æ®
    logger.info("ä¿å­˜è®­ç»ƒç»Ÿè®¡æ•°æ®...")
    
    # è®¡ç®—æ‰€æœ‰episodeçš„å¹³å‡å®Œæˆç‡ä½œä¸ºä»£è¡¨å€¼
    final_completion_stats = env.get_task_completion_rate()
    avg_completion_rate = final_completion_stats.get('on_time_completion_rate', 0.0)
    
    training_data = {
        'episode_rewards': metrics_tracker.episode_rewards,
        'episode_latencies': metrics_tracker.episode_delays,
        'episode_energies': metrics_tracker.episode_energy,
        'episode_completion_rates': [avg_completion_rate] * len(metrics_tracker.episode_rewards),
        'training_losses': training_losses,
        'global_step_count': global_step_count,
        'config': config
    }
    
    # ä¿å­˜JSONæ ¼å¼
    json_file = path_manager.get_data_file_path("json", f"llm_maddpg_training_stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(training_data, f, indent=2, ensure_ascii=False)
    
    # ğŸ†• ä¿å­˜æ ¸å¿ƒæŒ‡æ ‡åˆ°CSVè¡¨æ ¼
    logger.info("ä¿å­˜è®­ç»ƒæŒ‡æ ‡åˆ°CSVè¡¨æ ¼...")
    try:
        csv_filepath = save_training_metrics_csv(
            episode_rewards=metrics_tracker.episode_rewards,
            episode_latencies=metrics_tracker.episode_delays,
            episode_energies=metrics_tracker.episode_energy,
            episode_completion_rates=[avg_completion_rate] * len(metrics_tracker.episode_rewards),
            algorithm_name="LLM_MADDPG",
            save_dir=data_dir
        )
        logger.info(f"âœ… CSVæ–‡ä»¶å·²ä¿å­˜: {csv_filepath}")
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜CSVæ–‡ä»¶å¤±è´¥: {e}")
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    logger.info("ç»˜åˆ¶è®­ç»ƒæ›²çº¿...")
    
    # åˆ›å»ºç»¼åˆè®­ç»ƒæ›²çº¿
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # å¥–åŠ±æ›²çº¿
    axes[0, 0].plot(metrics_tracker.episode_rewards)
    axes[0, 0].set_title('Episode Rewards')
    axes[0, 0].set_xlabel('Episode')
    axes[0, 0].set_ylabel('Reward')
    axes[0, 0].grid(True)
    
    # æ—¶å»¶æ›²çº¿
    if metrics_tracker.episode_delays:
        axes[0, 1].plot(metrics_tracker.episode_delays)
        axes[0, 1].set_title('Episode Average Latency')
        axes[0, 1].set_xlabel('Episode')
        axes[0, 1].set_ylabel('Latency (s)')
        axes[0, 1].grid(True)
    
    # èƒ½è€—æ›²çº¿
    if metrics_tracker.episode_energy:
        axes[1, 0].plot(metrics_tracker.episode_energy)
        axes[1, 0].set_title('Episode Average Energy')
        axes[1, 0].set_xlabel('Episode')
        axes[1, 0].set_ylabel('Energy (J)')
        axes[1, 0].grid(True)
    
    # LLMä½¿ç”¨æ¯”ä¾‹æ›²çº¿
    if metrics_tracker.llm_used:
        axes[1, 1].plot(metrics_tracker.llm_used)
        axes[1, 1].set_title('LLM Usage Ratio')
        axes[1, 1].set_xlabel('Episode')
        axes[1, 1].set_ylabel('LLM Usage')
        axes[1, 1].grid(True)
    
    plt.tight_layout()
    plot_file = path_manager.get_plot_file_path("llm_maddpg_training_curves.png")
    plt.savefig(plot_file, dpi=300, bbox_inches='tight')
    plt.close()
    
    logger.info(f"è®­ç»ƒå®Œæˆï¼")
    logger.info(f"æ€»Episodes: {num_episodes}")
    logger.info(f"æ€»Steps: {global_step_count}")
    logger.info(f"æœ€ç»ˆå¹³å‡å¥–åŠ±: {np.mean(metrics_tracker.episode_rewards[-50:]):.3f}")
    logger.info(f"æœ€ç»ˆå¹³å‡æ—¶å»¶: {np.mean(metrics_tracker.episode_delays[-50:]):.3f}s")
    logger.info(f"æœ€ç»ˆå¹³å‡èƒ½è€—: {np.mean(metrics_tracker.episode_energy[-50:]):.3f}J")
    logger.info(f"æœ€ç»ˆLLMä½¿ç”¨æ¯”ä¾‹: {np.mean(metrics_tracker.llm_used[-50:]):.3f}")
    logger.info(f"æ¨¡å‹ä¿å­˜è·¯å¾„: {model_dir}")
    logger.info(f"æ•°æ®ä¿å­˜è·¯å¾„: {data_dir}")
    logger.info(f"ğŸ“ æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {path_manager.get_experiment_dir()}")
    
    # è¿”å›å®Œæ•´çš„è®­ç»ƒç»“æœ
    return {
        'algorithm': 'LLM+MADDPG',
        'episode_rewards': metrics_tracker.episode_rewards,
        'episode_latencies': metrics_tracker.episode_delays,
        'episode_energies': metrics_tracker.episode_energy,
        'episode_completion_rates': [avg_completion_rate] * len(metrics_tracker.episode_rewards),
        'training_losses': training_losses,
        'global_step_count': global_step_count,
        'config': config,
        'model_paths': [path_manager.get_model_file_path("llm_maddpg", f"agent_{i}_final.pth") for i in range(len(agents))]
    }


if __name__ == "__main__":
    config_path = "config.yaml"
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶
    if not os.path.exists(config_path):
        print(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        sys.exit(1)
    
    try:
        results = train_llm_maddpg_complete(config_path)
        print("\n" + "="*60)
        print("ğŸ‰ è®­ç»ƒæˆåŠŸå®Œæˆï¼")
        print("="*60)
        print(f"æœ€ç»ˆç»“æœ:")
        print(f"  å¹³å‡å¥–åŠ±: {np.mean(results['episode_rewards'][-50:]):.3f}")
        print(f"  å¹³å‡æ—¶å»¶: {np.mean(results['episode_latencies'][-50:]):.3f}s")
        print(f"  å¹³å‡èƒ½è€—: {np.mean(results['episode_energies'][-50:]):.3f}J")
        print(f"  ä»»åŠ¡å®Œæˆç‡: {np.mean(results['episode_completion_rates'][-50:]):.3f}")
        print("="*60)
        
    except Exception as e:
        print(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1) 