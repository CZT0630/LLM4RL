# experiments/train_llm_maddpg_complete.py
"""
å®Œæ•´ç‰ˆLLM+MADDPGè®­ç»ƒè„šæœ¬
å®ç°ï¼šæ¯stepæ–°ä»»åŠ¡ -> LLMå’¨è¯¢ï¼ˆäº¤æ›¿ï¼‰ -> AgentåŠ¨ä½œ -> æ‰§è¡Œ -> æ¯20æ­¥è®­ç»ƒä¸€æ¬¡
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import numpy as np
import torch
import yaml
import json
from tqdm import tqdm
import logging
from datetime import datetime
import matplotlib.pyplot as plt

from environment.cloud_edge_env import CloudEdgeDeviceEnv
from algos.maddpg_agent import MADDPGAgent
from algos.replay_buffer import ReplayBuffer
from llm_assistant.llm_client import LLMClient
from llm_assistant.prompt_builder import PromptBuilder
from llm_assistant.response_parser import ResponseParser
# from utils.plotting import plot_training_curves
# from utils.metrics import calculate_episode_metrics


def setup_logging():
    """è®¾ç½®æ—¥å¿—"""
    log_dir = "results/logs"
    os.makedirs(log_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = f"{log_dir}/train_complete_{timestamp}.log"
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(__name__)


def create_agents(env, config):
    """åˆ›å»ºæ‰€æœ‰Agent"""
    agents = []
    # ä½¿ç”¨æ­£ç¡®çš„å•ä¸ªAgentçŠ¶æ€ç»´åº¦
    state_dim = env.get_agent_state_dim()  # 20ç»´ï¼š3(è‡ªå·±UE) + 10(æ‰€æœ‰ES) + 1(CS) + 6(è‡ªå·±ä»»åŠ¡)
    action_dim = 4  # [Î±1, Î±2, Î±3, edge_id]
    
    print(f"ğŸ”§ Agenté…ç½®ä¿¡æ¯:")
    print(f"  å•ä¸ªAgentçŠ¶æ€ç»´åº¦: {state_dim}")
    print(f"  å…¨å±€çŠ¶æ€ç»´åº¦: {env.observation_space.shape[0]}")
    print(f"  åŠ¨ä½œç»´åº¦: {action_dim}")
    print(f"  è®¾å¤‡æ•°é‡: {env.num_devices}")
    
    for i in range(env.num_devices):
        agent = MADDPGAgent(
            state_dim=state_dim,
            action_dim=action_dim,
            num_agents=env.num_devices,
            agent_idx=i,
            config=config['training']
        )
        agents.append(agent)
        
    return agents


def consult_llm_for_all_devices(env, llm_client, prompt_builder, response_parser, logger):
    """ä¸ºæ‰€æœ‰è®¾å¤‡å’¨è¯¢LLMè·å–ä¸“å®¶åŠ¨ä½œ"""
    try:
        # è·å–ç³»ç»ŸçŠ¶æ€ä¿¡æ¯
        device_info = env.get_device_info()
        edge_info = env.get_edge_info()
        cloud_info = env.get_cloud_info()
        task_info = env.get_current_tasks_info()
        
        # æ„å»ºLLMæç¤º
        prompt = prompt_builder.build_offloading_strategy_prompt(
            env_state=None,  # ç¯å¢ƒçŠ¶æ€ä¿¡æ¯
            device_info=device_info,
            edge_info=edge_info,
            cloud_info=cloud_info,
            tasks_info=task_info
        )
        
        # å’¨è¯¢LLM
        response = llm_client.query(prompt)
        
        # è§£æLLMå“åº”
        llm_actions = response_parser.parse_unload_strategy(response, env.num_devices, env.num_edges, env.num_clouds)
        
        # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼ [Î±1, Î±2, Î±3, edge_id]
        formatted_actions = []
        for i in range(env.num_devices):
            if i < len(llm_actions):
                action = llm_actions[i]
                # å°†ä¸‰å…ƒåˆ†å‰²å†³ç­–è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                alpha1 = action.get('local_ratio', 0.0)
                alpha2 = action.get('edge_ratio', 0.0)
                alpha3 = action.get('cloud_ratio', 1.0)
                edge_id = action.get('edge_server_id', 0)
                
                formatted_actions.append([alpha1, alpha2, alpha3, edge_id])
            else:
                # é»˜è®¤åŠ¨ä½œï¼šå…¨éƒ¨åœ¨äº‘ç«¯æ‰§è¡Œ
                formatted_actions.append([0.0, 0.0, 1, 0])
                
        return formatted_actions
        
    except Exception as e:
        logger.warning(f"LLMå’¨è¯¢å¤±è´¥: {e}")
        # è¿”å›é»˜è®¤åŠ¨ä½œ
        default_actions = [[0, 0, 1, 0] for _ in range(env.num_devices)]
        return default_actions


def train_agents_from_buffer(agents, shared_buffer, logger, step_count):
    """ä»å…±äº«ç¼“å†²åŒºè®­ç»ƒæ‰€æœ‰Agent"""
    if len(shared_buffer) < 64:  # æœ€å°æ‰¹é‡è¦æ±‚
        logger.debug(f"ç¼“å†²åŒºæ ·æœ¬ä¸è¶³ï¼Œè·³è¿‡è®­ç»ƒ (å½“å‰: {len(shared_buffer)})")
        return {}
    
    training_stats = {}
    
    for i, agent in enumerate(agents):
        try:
            stats = agent.train(all_agents=agents, replay_buffer=shared_buffer)
            training_stats[f'agent_{i}'] = stats
            
            if stats:
                logger.debug(f"Agent{i} è®­ç»ƒå®Œæˆ: "
                           f"critic_loss={stats.get('critic_loss', 0):.4f}, "
                           f"actor_loss={stats.get('actor_loss', 0):.4f}, "
                           f"distill_loss={stats.get('distill_loss', 0):.4f}")
                           
        except Exception as e:
            logger.error(f"Agent{i} è®­ç»ƒå¤±è´¥: {e}")
            
    logger.info(f"Step {step_count}: å®Œæˆæ‰€æœ‰Agentè®­ç»ƒ")
    return training_stats


def train_llm_maddpg_complete(config_path):
    """ä¸»è®­ç»ƒå‡½æ•° - å®Œæ•´çš„è®­ç»ƒæµç¨‹"""
    # è®¾ç½®æ—¥å¿—
    logger = setup_logging()
    logger.info("å¼€å§‹å®Œæ•´ç‰ˆLLM+MADDPGè®­ç»ƒ")
    
    # åŠ è½½é…ç½®
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # åˆ›å»ºç¯å¢ƒ
    logger.info("åˆ›å»ºäº‘è¾¹ç«¯ä¸‰å±‚æ¶æ„ç¯å¢ƒ...")
    env = CloudEdgeDeviceEnv(config)
    
    # åˆ›å»ºAgent
    logger.info("åˆ›å»ºMADDPGæ™ºèƒ½ä½“...")
    agents = create_agents(env, config)
    
    # åˆ›å»ºå…±äº«ç»éªŒå›æ”¾ç¼“å†²åŒº
    buffer_size = config['training'].get('buffer_size', 100000)
    shared_buffer = ReplayBuffer(buffer_size)
    
    # åˆ›å»ºLLMç»„ä»¶
    logger.info("åˆå§‹åŒ–LLMåŠ©æ‰‹...")
    try:
        llm_client = LLMClient(
            api_key=config['llm'].get('api_key', ''),
            model_name=config['llm']['model_name'],
            server_url=config['llm'].get('base_url', 'http://10.200.1.35:8888/v1/completions'),
            timeout_connect=config['llm'].get('timeout', 120),
            timeout_read=config['llm'].get('read_timeout', 300),
            use_mock=config['llm'].get('use_mock_when_unavailable', False),
            config=config  # ä¼ é€’å®Œæ•´é…ç½®ä»¥æ”¯æŒmax_tokensç­‰å‚æ•°
        )
        prompt_builder = PromptBuilder()
        response_parser = ResponseParser()
        llm_available = True
    except Exception as e:
        logger.warning(f"LLMåˆå§‹åŒ–å¤±è´¥ï¼Œå°†è·³è¿‡LLMå’¨è¯¢: {e}")
        llm_available = False
    
    # è®­ç»ƒå‚æ•°
    num_episodes = config['training'].get('episodes', 1000)
    max_steps_per_episode = config['training'].get('max_steps_per_episode', 100)
    
    # è®­ç»ƒç­–ç•¥å‚æ•°
    train_frequency = 20  # æ¯20æ­¥è®­ç»ƒä¸€æ¬¡
    llm_episode_interval = 2  # æ¯2ä¸ªEpisodeä½¿ç”¨ä¸€æ¬¡LLMï¼ˆäº¤æ›¿ï¼‰
    
    # è®°å½•æŒ‡æ ‡
    episode_rewards = []
    episode_latencies = []
    episode_energies = []
    episode_completion_rates = []
    training_losses = []
    
    # å…¨å±€stepè®¡æ•°å™¨
    global_step_count = 0
    
    logger.info(f"å¼€å§‹è®­ç»ƒï¼Œæ€»Episodes: {num_episodes}")
    logger.info(f"è®­ç»ƒç­–ç•¥: æ¯{train_frequency}æ­¥è®­ç»ƒä¸€æ¬¡, æ¯{llm_episode_interval}ä¸ªEpisodeä½¿ç”¨LLM")
    
    for episode in tqdm(range(num_episodes), desc="è®­ç»ƒè¿›åº¦"):
        logger.info(f"\n{'='*80}")
        logger.info(f"Episode {episode + 1}/{num_episodes}")
        logger.info(f"{'='*80}")
        
        # åˆ¤æ–­æ˜¯å¦åœ¨å½“å‰Episodeä½¿ç”¨LLM
        use_llm_this_episode = (episode % llm_episode_interval == 0) and llm_available
        logger.info(f"Episode {episode + 1}: {'ä½¿ç”¨LLMæŒ‡å¯¼' if use_llm_this_episode else 'çº¯MADDPGè®­ç»ƒ'}")
        
        # é‡ç½®ç¯å¢ƒ
        state, _ = env.reset()
        episode_reward = 0
        episode_step_count = 0
        
        # Episodeå¾ªç¯
        for step in range(max_steps_per_episode):
            global_step_count += 1
            episode_step_count += 1
            
            logger.debug(f"\nEpisode {episode + 1}, Step {step + 1}")

            # å½“å‰stepçš„çŠ¶æ€
            current_state = state.copy()
            
            # 1. LLMå’¨è¯¢ï¼ˆå¦‚æœå½“å‰Episodeä½¿ç”¨LLMï¼‰
            llm_expert_actions = None
            if use_llm_this_episode:
                llm_expert_actions = consult_llm_for_all_devices(
                    env, llm_client, prompt_builder, response_parser, logger
                )
                logger.debug(f"è·å–LLMä¸“å®¶åŠ¨ä½œ: {len(llm_expert_actions)}ä¸ªè®¾å¤‡")
            
            # 2. AgentåŠ¨ä½œç”Ÿæˆ
            agent_actions = []
            print(f"\nğŸ“‹ MADDPGæ™ºèƒ½ä½“ç­–ç•¥ç”Ÿæˆ:")
            print(f"{'='*80}")
            
            for i, agent in enumerate(agents):
                # ä½¿ç”¨æ­£ç¡®çš„AgentçŠ¶æ€æå–æ–¹æ³•
                agent_state = env.extract_agent_state(current_state, i)
                
                # åœ¨è®­ç»ƒæ—©æœŸå¢åŠ æ¢ç´¢
                add_noise = episode < num_episodes * 0.8
                action = agent.select_action(agent_state, add_noise=add_noise)
                agent_actions.append(action)
                
                # è¯¦ç»†è¾“å‡ºMADDPGç­–ç•¥
                alpha1, alpha2, alpha3 = action[:3]
                edge_id = int(action[-1]) if len(action) >= 4 else 0
                
                # å½’ä¸€åŒ–åˆ†å‰²æ¯”ä¾‹
                total = alpha1 + alpha2 + alpha3
                if total > 0:
                    alpha1_norm, alpha2_norm, alpha3_norm = alpha1/total, alpha2/total, alpha3/total
                else:
                    alpha1_norm, alpha2_norm, alpha3_norm = 1.0, 0.0, 0.0
                
                print(f"  ğŸ¤– Agent{i} (Device{i}) MADDPGç­–ç•¥:")
                print(f"    AgentçŠ¶æ€ç»´åº¦: {len(agent_state)} (æ­£ç¡®æå–)")
                print(f"    åŸå§‹åŠ¨ä½œ: [Î±1={alpha1:.3f}, Î±2={alpha2:.3f}, Î±3={alpha3:.3f}, edge={action[-1]:.3f}]")
                print(f"    å½’ä¸€åŒ–åˆ†å‰²: [æœ¬åœ°:{alpha1_norm:.3f}, è¾¹ç¼˜:{alpha2_norm:.3f}, äº‘ç«¯:{alpha3_norm:.3f}]")
                print(f"    ç›®æ ‡è¾¹ç¼˜æœåŠ¡å™¨: Edge{edge_id}")
                print(f"    æ¢ç´¢æ¨¡å¼: {'å¼€å¯' if add_noise else 'å…³é—­'}")
                
                # # ç­–ç•¥åˆ†æ
                # if alpha1_norm > 0.5:
                #     strategy = "æœ¬åœ°ä¼˜å…ˆç­–ç•¥"
                # elif alpha2_norm > 0.5:
                #     strategy = "è¾¹ç¼˜å¸è½½ç­–ç•¥"
                # elif alpha3_norm > 0.5:
                #     strategy = "äº‘ç«¯å¸è½½ç­–ç•¥"
                # else:
                #     strategy = "æ··åˆå¸è½½ç­–ç•¥"
                # print(f"    ç­–ç•¥ç±»å‹: {strategy}")
                # print()

            agent_actions = np.array(agent_actions)
            
            # print(f"ğŸ“Š MADDPGæ•´ä½“ç­–ç•¥ç»Ÿè®¡:")
            # print(f"  æ€»è®¾å¤‡æ•°: {len(agent_actions)}")
            # print(f"  å¹³å‡æœ¬åœ°æ¯”ä¾‹: {np.mean(agent_actions[:, 0]):.3f}")
            # print(f"  å¹³å‡è¾¹ç¼˜æ¯”ä¾‹: {np.mean(agent_actions[:, 1]):.3f}")
            # print(f"  å¹³å‡äº‘ç«¯æ¯”ä¾‹: {np.mean(agent_actions[:, 2]):.3f}")
            # print(f"  æœ€å¸¸é€‰æ‹©çš„è¾¹ç¼˜æœåŠ¡å™¨: Edge{int(np.round(np.mean(agent_actions[:, -1])))}")
            # print()
            
            # 3. æ‰§è¡ŒåŠ¨ä½œã€ç¯å¢ƒäº¤äº’
            next_state, rewards, terminated, truncated, info = env.step(
                agent_actions, llm_actions=llm_expert_actions
            )
            
            # 4. å­˜å‚¨ç»éªŒåˆ°å…±äº«ç¼“å†²åŒº
            for i in range(env.num_devices):
                # ä½¿ç”¨æ­£ç¡®çš„AgentçŠ¶æ€æå–æ–¹æ³•
                agent_state = env.extract_agent_state(current_state, i)
                agent_next_state = env.extract_agent_state(next_state, i)
                
                shared_buffer.add(
                    state=agent_state,
                    action=agent_actions[i],
                    reward=rewards[i],
                    next_state=agent_next_state,
                    done=terminated or truncated,
                    llm_action=llm_expert_actions if use_llm_this_episode else None
                )
            
            # 5. æ¯20æ­¥è®­ç»ƒä¸€æ¬¡
            if global_step_count % train_frequency == 0:
                logger.info(f"\n--- ç¬¬{global_step_count}æ­¥: å¼€å§‹è®­ç»ƒæ‰€æœ‰Agent ---")
                train_stats = train_agents_from_buffer(agents, shared_buffer, logger, global_step_count)
                training_losses.append(train_stats)
            
            # æ›´æ–°çŠ¶æ€å’Œå¥–åŠ±
            state = next_state
            episode_reward += np.mean(rewards)
            
            # æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶
            if terminated or truncated:
                logger.info(f"Episode {episode + 1} åœ¨ç¬¬{step + 1}æ­¥ç»ˆæ­¢")
                break
        
        # Episodeç»“æŸï¼Œè®°å½•æŒ‡æ ‡
        episode_rewards.append(episode_reward)
        
        # ä»infoä¸­æå–æŒ‡æ ‡
        if info:
            avg_latency = np.mean(info.get('total_latencies', [0]))
            avg_energy = np.mean(info.get('total_energies', [0]))
            
            episode_latencies.append(avg_latency)
            episode_energies.append(avg_energy)
            
            # è®¡ç®—ä»»åŠ¡å®Œæˆç‡
            completion_stats = info.get('task_completion_stats', {})
            completion_rate = completion_stats.get('on_time_completion_rate', 0.0)
            overall_completion_rate = completion_stats.get('overall_completion_rate', 0.0)
            timeout_rate = completion_stats.get('timeout_rate', 0.0)
            
            episode_completion_rates.append(completion_rate)
            
            # å¦‚æœæœ‰æˆªæ­¢æ—¶é—´è¿åä¿¡æ¯ï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯
            violations = info.get('deadline_violations', [])
            if violations:
                logger.info(f"Episode {episode + 1} æˆªæ­¢æ—¶é—´è¿å:")
                for v in violations[-3:]:  # åªæ˜¾ç¤ºæœ€è¿‘3ä¸ªè¿å
                    logger.info(f"  ä»»åŠ¡{v['task_id']}({v['task_type']}): æˆªæ­¢{v['deadline']:.1f}s, å®é™…{v['actual_time']:.1f}s, è¶…æ—¶{v['overtime']:.1f}s")
        
        # å®šæœŸæ‰“å°è¿›åº¦
        if (episode + 1) % 10 == 0:
            recent_reward = np.mean(episode_rewards[-10:])
            recent_latency = np.mean(episode_latencies[-10:]) if episode_latencies else 0
            recent_energy = np.mean(episode_energies[-10:]) if episode_energies else 0
            recent_completion = np.mean(episode_completion_rates[-10:]) if episode_completion_rates else 0
            
            logger.info(f"\nEpisode {episode + 1} é˜¶æ®µæ€§æ€»ç»“:")
            logger.info(f"  æœ€è¿‘10è½®å¹³å‡å¥–åŠ±: {recent_reward:.3f}")
            logger.info(f"  æœ€è¿‘10è½®å¹³å‡æ—¶å»¶: {recent_latency:.3f}s")
            logger.info(f"  æœ€è¿‘10è½®å¹³å‡èƒ½è€—: {recent_energy:.3f}J")
            logger.info(f"  æœ€è¿‘10è½®æŒ‰æ—¶å®Œæˆç‡: {recent_completion:.3f}")
            
            # æ˜¾ç¤ºè¯¦ç»†çš„ä»»åŠ¡å®Œæˆç»Ÿè®¡
            if info and 'task_completion_stats' in info:
                comp_stats = info['task_completion_stats']
                logger.info(f"  è¯¦ç»†å®Œæˆç‡ç»Ÿè®¡:")
                logger.info(f"    æ€»å®Œæˆç‡: {comp_stats.get('overall_completion_rate', 0):.3f}")
                logger.info(f"    æŒ‰æ—¶å®Œæˆç‡: {comp_stats.get('on_time_completion_rate', 0):.3f}")
                logger.info(f"    è¶…æ—¶å®Œæˆç‡: {comp_stats.get('timeout_rate', 0):.3f}")
                logger.info(f"    å¤±è´¥ç‡: {comp_stats.get('failure_rate', 0):.3f}")
                logger.info(f"    å¹³å‡è¶…æ—¶æ—¶é—´: {comp_stats.get('avg_overtime', 0):.2f}s")
            
            logger.info(f"  å…¨å±€æ­¥æ•°: {global_step_count}")
            logger.info(f"  ç¼“å†²åŒºå¤§å°: {len(shared_buffer)}")
        
        # å®šæœŸä¿å­˜æ¨¡å‹
        if (episode + 1) % 100 == 0:
            model_dir = "results/models"
            os.makedirs(model_dir, exist_ok=True)
            for i, agent in enumerate(agents):
                model_path = f"{model_dir}/complete_agent_{i}_episode_{episode + 1}.pth"
                agent.save_model(model_path)
            logger.info(f"Episode {episode + 1}: æ¨¡å‹å·²ä¿å­˜")
    
    # è®­ç»ƒç»“æŸï¼Œä¿å­˜æœ€ç»ˆæ¨¡å‹
    logger.info("è®­ç»ƒå®Œæˆï¼Œä¿å­˜æœ€ç»ˆæ¨¡å‹...")
    final_model_dir = "results/final_models"
    os.makedirs(final_model_dir, exist_ok=True)
    for i, agent in enumerate(agents):
        model_path = f"{final_model_dir}/complete_agent_{i}_final.pth"
        agent.save_model(model_path)
    
    # ä¿å­˜è®­ç»ƒç»Ÿè®¡æ•°æ®
    logger.info("ä¿å­˜è®­ç»ƒç»Ÿè®¡æ•°æ®...")
    stats_dir = "results/stats"
    os.makedirs(stats_dir, exist_ok=True)
    
    training_data = {
        'episode_rewards': episode_rewards,
        'episode_latencies': episode_latencies,
        'episode_energies': episode_energies,
        'episode_completion_rates': episode_completion_rates,
        'training_losses': training_losses,
        'global_step_count': global_step_count,
        'config': config
    }
    
    with open(f"{stats_dir}/training_stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json", 'w', encoding='utf-8') as f:
        json.dump(training_data, f, indent=2, ensure_ascii=False)
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    logger.info("ç»˜åˆ¶è®­ç»ƒæ›²çº¿...")
    plot_dir = "results/plots"
    os.makedirs(plot_dir, exist_ok=True)
    
    # åˆ›å»ºç»¼åˆè®­ç»ƒæ›²çº¿
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # å¥–åŠ±æ›²çº¿
    axes[0, 0].plot(episode_rewards)
    axes[0, 0].set_title('Episode Rewards')
    axes[0, 0].set_xlabel('Episode')
    axes[0, 0].set_ylabel('Reward')
    axes[0, 0].grid(True)
    
    # æ—¶å»¶æ›²çº¿
    if episode_latencies:
        axes[0, 1].plot(episode_latencies)
        axes[0, 1].set_title('Episode Average Latency')
        axes[0, 1].set_xlabel('Episode')
        axes[0, 1].set_ylabel('Latency (s)')
        axes[0, 1].grid(True)
    
    # èƒ½è€—æ›²çº¿
    if episode_energies:
        axes[1, 0].plot(episode_energies)
        axes[1, 0].set_title('Episode Average Energy')
        axes[1, 0].set_xlabel('Episode')
        axes[1, 0].set_ylabel('Energy (J)')
        axes[1, 0].grid(True)
    
    # ä»»åŠ¡å®Œæˆç‡æ›²çº¿
    if episode_completion_rates:
        axes[1, 1].plot(episode_completion_rates)
        axes[1, 1].set_title('Task Completion Rate')
        axes[1, 1].set_xlabel('Episode')
        axes[1, 1].set_ylabel('Completion Rate')
        axes[1, 1].grid(True)
    
    plt.tight_layout()
    plt.savefig(f"{plot_dir}/complete_training_curves.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    logger.info(f"è®­ç»ƒå®Œæˆï¼")
    logger.info(f"æ€»Episodes: {num_episodes}")
    logger.info(f"æ€»Steps: {global_step_count}")
    logger.info(f"æœ€ç»ˆå¹³å‡å¥–åŠ±: {np.mean(episode_rewards[-50:]):.3f}")
    logger.info(f"æœ€ç»ˆå¹³å‡æ—¶å»¶: {np.mean(episode_latencies[-50:]):.3f}s")
    logger.info(f"æœ€ç»ˆå¹³å‡èƒ½è€—: {np.mean(episode_energies[-50:]):.3f}J")
    logger.info(f"æœ€ç»ˆä»»åŠ¡å®Œæˆç‡: {np.mean(episode_completion_rates[-50:]):.3f}")
    logger.info(f"ç»“æœä¿å­˜åœ¨ results/ ç›®å½•")
    
    return training_data


if __name__ == "__main__":
    config_path = "config.yaml"
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶
    if not os.path.exists(config_path):
        print(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        sys.exit(1)
    
    try:
        results = train_llm_maddpg_complete(config_path)
        print("\n" + "="*60)
        print("ğŸ‰ è®­ç»ƒæˆåŠŸå®Œæˆï¼")
        print("="*60)
        print(f"æœ€ç»ˆç»“æœ:")
        print(f"  å¹³å‡å¥–åŠ±: {np.mean(results['episode_rewards'][-50:]):.3f}")
        print(f"  å¹³å‡æ—¶å»¶: {np.mean(results['episode_latencies'][-50:]):.3f}s")
        print(f"  å¹³å‡èƒ½è€—: {np.mean(results['episode_energies'][-50:]):.3f}J")
        print(f"  ä»»åŠ¡å®Œæˆç‡: {np.mean(results['episode_completion_rates'][-50:]):.3f}")
        print("="*60)
        
    except Exception as e:
        print(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1) 