import numpy as np
import torch
import gymnasium as gym
from environment.cloud_edge_env import CloudEdgeDeviceEnv
from llm_assistant.llm_client import LLMClient
from llm_assistant.response_parser import ResponseParser
from utils.config import load_config
from utils.path_manager import get_path_manager
from utils.csv_saver import save_test_results_csv

def test_llm(model_path=None, config=None):
    if config is None:
        config = load_config()
    
    # ä½¿ç”¨è·¯å¾„ç®¡ç†å™¨
    path_manager = get_path_manager()
    
    print(f"ğŸ”§ [çº¯LLMæµ‹è¯•] é…ç½®ä¿¡æ¯:")
    print(f"  æµ‹è¯•ç»“æœä¿å­˜è·¯å¾„: {path_manager.get_test_results_path()}")
    print(f"  ğŸ§  æ³¨æ„: çº¯LLMæµ‹è¯•ç›´æ¥ä½¿ç”¨LLMè¿›è¡Œå†³ç­–")
    
    env = CloudEdgeDeviceEnv(config['environment'])
    
    llm_client = LLMClient(
        config=config,
        use_mock=config['llm'].get('use_mock_when_unavailable', True)
    )
    
    num_episodes = config['testing']['num_episodes']
    max_steps = config['maddpg']['max_steps']
    num_agents = env.num_devices
    all_episode_energy = []
    all_episode_delay = []
    all_episode_util = []
    
    print(f"  è®¾å¤‡æ•°é‡: {num_agents}")
    print(f"  æµ‹è¯•è½®æ•°: {num_episodes}")
    print(f"  æ¯è½®æœ€å¤§æ­¥æ•°: {max_steps}")
    
    print(f"\nğŸ§ª å¼€å§‹æµ‹è¯•ï¼Œå…±{num_episodes}è½®...")
    
    for episode in range(num_episodes):
        state, _ = env.reset()
        episode_reward = 0
        episode_energy = 0
        episode_delay = 0
        episode_util = 0
        step_count = 0
        
        for step in range(max_steps):
            device_info = [{"cpu": d.cpu_frequency} for d in env.devices]
            edge_info = [{"cpu": e.cpu_frequency} for e in env.edge_servers]
            cloud_info = [{"cpu": c.cpu_frequency} for c in env.cloud_servers]
            
            # è·å–æ ¼å¼1çš„LLMç­–ç•¥
            llm_strategies = llm_client.get_unload_strategy(state, device_info, edge_info, cloud_info)
            
            # è§£æLLMå“åº”ï¼ˆæ ¼å¼1ï¼‰
            parsed_strategies = ResponseParser.parse_unload_strategy(
                llm_strategies,
                env.num_devices,
                env.num_edges,
                env.num_clouds
            )
            
            # å°†æ ¼å¼1ç­–ç•¥è½¬æ¢ä¸ºç¯å¢ƒå¯æ¥å—çš„åŠ¨ä½œæ ¼å¼ [Î±1, Î±2, Î±3, edge_id]
            action_list = []
            for device_idx in range(env.num_devices):
                device_strategy = next(
                    (s for s in parsed_strategies if s["device_id"] == device_idx), 
                    {
                        "device_id": device_idx,
                        "local_ratio": 1.0,
                        "edge_ratio": 0.0,
                        "cloud_ratio": 0.0,
                        "target_edge": 0
                    }
                )
                
                # è½¬æ¢ä¸ºç¯å¢ƒåŠ¨ä½œæ ¼å¼
                action = [
                    device_strategy["local_ratio"],   # Î±1: æœ¬åœ°æ‰§è¡Œæ¯”ä¾‹
                    device_strategy["edge_ratio"],    # Î±2: è¾¹ç¼˜æ‰§è¡Œæ¯”ä¾‹  
                    device_strategy["cloud_ratio"],   # Î±3: äº‘ç«¯æ‰§è¡Œæ¯”ä¾‹
                    device_strategy["target_edge"]    # edge_id: ç›®æ ‡è¾¹ç¼˜æœåŠ¡å™¨
                ]
                action_list.append(action)

            actions = np.array(action_list)

            next_state, rewards, terminated, truncated, info = env.step(actions)
            done = terminated or truncated
            episode_energy += sum(info['energies'])
            episode_delay += sum(info['delays'])
            episode_util += sum(info['utilizations'])
            step_count += 1
            state = next_state
            episode_reward += sum(rewards)
            
            if done:
                break
                
        avg_energy = episode_energy / num_agents
        avg_delay = episode_delay / num_agents
        avg_util = episode_util / (num_agents * step_count) if step_count > 0 else 0
        all_episode_energy.append(avg_energy)
        all_episode_delay.append(avg_delay)
        all_episode_util.append(avg_util)
        
        if (episode + 1) % 10 == 0:
            print(f"[çº¯LLM] Episode {episode + 1}/{num_episodes} - "
                  f"èƒ½è€—: {avg_energy:.4f}, åˆ©ç”¨ç‡: {avg_util:.4f}, æ—¶å»¶: {avg_delay:.4f}")

    # è®¡ç®—æœ€ç»ˆç»Ÿè®¡ç»“æœ
    final_energy = np.mean(all_episode_energy)
    final_util = np.mean(all_episode_util)
    final_delay = np.mean(all_episode_delay)

    print(f"\nğŸ“Š [çº¯LLM] æµ‹è¯•å®Œæˆ!")
    print(f"  å¹³å‡èƒ½è€—: {final_energy:.4f}")
    print(f"  å¹³å‡èµ„æºåˆ©ç”¨ç‡: {final_util:.4f}")
    print(f"  å¹³å‡ä»»åŠ¡æ—¶å»¶: {final_delay:.4f}")
    
    # ä¿å­˜æµ‹è¯•ç»“æœ
    try:
        test_results = {
            'algorithm': 'LLM',
            'description': 'çº¯LLMç®—æ³•çš„æµ‹è¯•ç»“æœ',
            'num_episodes': num_episodes,
            'avg_energy': final_energy,
            'avg_utilization': final_util,
            'avg_delay': final_delay,
            'energy_std': np.std(all_episode_energy),
            'utilization_std': np.std(all_episode_util),
            'delay_std': np.std(all_episode_delay),
            'all_episode_energy': all_episode_energy,
            'all_episode_utilization': all_episode_util,
            'all_episode_delay': all_episode_delay
        }
        
        # ä¿å­˜åˆ°æµ‹è¯•ç»“æœç›®å½•
        import json
        result_file = path_manager.get_test_results_file_path("llm_test_results.json")
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(test_results, f, indent=2, ensure_ascii=False, default=str)
        print(f"  âœ… æµ‹è¯•ç»“æœä¿å­˜è‡³: {result_file}")
        
        # ä¿å­˜CSVæ ¼å¼çš„æµ‹è¯•ç»“æœ
        csv_file = save_test_results_csv(
            test_results=test_results,
            algorithm_name="LLM",
            save_dir=path_manager.get_test_results_path()
        )
        print(f"  âœ… CSVç»“æœä¿å­˜è‡³: {csv_file}")
        
    except Exception as e:
        print(f"  âŒ ä¿å­˜æµ‹è¯•ç»“æœå¤±è´¥: {e}")
        
    return all_episode_energy, all_episode_util, all_episode_delay

if __name__ == "__main__":
    test_llm() 