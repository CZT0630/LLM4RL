"""
LLMå“åº”è§£ææµ‹è¯•è„šæœ¬
æµ‹è¯•æ”¹è¿›åçš„promptå’Œè§£æåŠŸèƒ½
"""
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from llm_assistant.llm_client import LLMClient
from llm_assistant.response_parser import ResponseParser
from utils.config import load_config
import numpy as np

def test_llm_parsing():
    """æµ‹è¯•LLMçš„promptå’Œå“åº”è§£æ"""
    print("ğŸ§ª å¼€å§‹æµ‹è¯•LLMè§£æåŠŸèƒ½...")
    
    # åŠ è½½é…ç½®
    config = load_config()
    
    # åˆ›å»ºLLMå®¢æˆ·ç«¯
    llm_client = LLMClient(
        model_name=config['llm']['model_name'],
        server_url=config['llm']['server_url'],
        timeout_connect=config['llm'].get('timeout_connect', 120),
        timeout_read=config['llm'].get('timeout_read', 300),
        use_mock=False  # å¼ºåˆ¶ä½¿ç”¨çœŸå®LLMæµ‹è¯•
    )
    
    # æ¨¡æ‹Ÿç¯å¢ƒçŠ¶æ€
    device_info = [
        {"cpu": 2.0, "memory": 4.0},
        {"cpu": 2.0, "memory": 4.0},
        {"cpu": 2.0, "memory": 4.0}
    ]
    edge_info = [
        {"cpu": 8.0, "memory": 16.0},
        {"cpu": 8.0, "memory": 16.0}
    ]
    cloud_info = [
        {"cpu": 32.0, "memory": 64.0}
    ]
    
    # æ„é€ ç®€å•çš„ç¯å¢ƒçŠ¶æ€
    env_state = np.concatenate([
        # è®¾å¤‡çŠ¶æ€ (3ä¸ªè®¾å¤‡, æ¯ä¸ª4ä¸ªå€¼)
        [0.0, 0.0, 1.0, 0.0] * 3,  # è®¾å¤‡çŠ¶æ€
        # è¾¹ç¼˜æœåŠ¡å™¨çŠ¶æ€ (2ä¸ªè¾¹ç¼˜, æ¯ä¸ª3ä¸ªå€¼)
        [0.0, 0.0, 0.0] * 2,  # è¾¹ç¼˜çŠ¶æ€
        # äº‘æœåŠ¡å™¨çŠ¶æ€ (1ä¸ªäº‘, 3ä¸ªå€¼)
        [0.0, 0.0, 0.0],  # äº‘çŠ¶æ€
        # ä»»åŠ¡çŠ¶æ€ (3ä¸ªä»»åŠ¡, æ¯ä¸ª3ä¸ªå€¼)
        [400.0, 50.0, 30.0],  # ä»»åŠ¡0: 400MI, 50MB, 30s
        [800.0, 20.0, 45.0],  # ä»»åŠ¡1: 800MI, 20MB, 45s
        [200.0, 80.0, 60.0],  # ä»»åŠ¡2: 200MI, 80MB, 60s
    ])
    
    print("\nğŸ“¤ æµ‹è¯•LLMè¯·æ±‚å’Œå“åº”...")
    
    try:
        # è·å–LLMç­–ç•¥
        strategies = llm_client.get_unload_strategy(env_state, device_info, edge_info, cloud_info)
        
        print(f"\nâœ… LLMç­–ç•¥è·å–æˆåŠŸï¼")
        print(f"ğŸ“‹ è·å¾— {len(strategies)} ä¸ªå¸è½½ç­–ç•¥:")
        
        for strategy in strategies:
            task_id = strategy.get('task_id', 'æœªçŸ¥')
            offload_ratio = strategy.get('offload_ratio', 0.0)
            target_node = strategy.get('target_node', 0)
            
            target_name = "æœ¬åœ°"
            if target_node == 1:
                target_name = "è¾¹ç¼˜æœåŠ¡å™¨0"
            elif target_node == 2:
                target_name = "è¾¹ç¼˜æœåŠ¡å™¨1"
            elif target_node == 3:
                target_name = "äº‘æœåŠ¡å™¨"
                
            print(f"  ä»»åŠ¡{task_id}: å¸è½½æ¯”ä¾‹={offload_ratio:.2f}, ç›®æ ‡={target_name}")
        
        # ä½¿ç”¨ResponseParserè¿›ä¸€æ­¥éªŒè¯
        print(f"\nğŸ” ä½¿ç”¨ResponseParseréªŒè¯è§£æç»“æœ...")
        parsed_strategies = ResponseParser.parse_unload_strategy(
            strategies, len(device_info), len(edge_info), len(cloud_info)
        )
        
        if len(parsed_strategies) == len(device_info):
            print(f"âœ… è§£æéªŒè¯æˆåŠŸï¼æ‰€æœ‰{len(device_info)}ä¸ªä»»åŠ¡éƒ½æœ‰å¯¹åº”ç­–ç•¥")
        else:
            print(f"âš ï¸ è§£æéªŒè¯è­¦å‘Šï¼šæœŸæœ›{len(device_info)}ä¸ªç­–ç•¥ï¼Œå®é™…å¾—åˆ°{len(parsed_strategies)}ä¸ª")
            
        return True
        
    except Exception as e:
        print(f"âŒ LLMæµ‹è¯•å¤±è´¥: {e}")
        
        print(f"\nğŸ¤– å›é€€åˆ°æ¨¡æ‹Ÿç­–ç•¥æµ‹è¯•...")
        # æµ‹è¯•æ¨¡æ‹Ÿç­–ç•¥ç”Ÿæˆ
        mock_strategies = llm_client._generate_mock_strategies(env_state, device_info, edge_info, cloud_info)
        
        print(f"âœ… æ¨¡æ‹Ÿç­–ç•¥ç”ŸæˆæˆåŠŸï¼")
        print(f"ğŸ“‹ è·å¾— {len(mock_strategies)} ä¸ªæ¨¡æ‹Ÿç­–ç•¥:")
        
        for strategy in mock_strategies:
            task_id = strategy.get('task_id', 'æœªçŸ¥')
            offload_ratio = strategy.get('offload_ratio', 0.0)
            target_node = strategy.get('target_node', 0)
            
            target_name = "æœ¬åœ°"
            if target_node == 1:
                target_name = "è¾¹ç¼˜æœåŠ¡å™¨0"
            elif target_node == 2:
                target_name = "è¾¹ç¼˜æœåŠ¡å™¨1"
            elif target_node == 3:
                target_name = "äº‘æœåŠ¡å™¨"
                
            print(f"  ä»»åŠ¡{task_id}: å¸è½½æ¯”ä¾‹={offload_ratio:.2f}, ç›®æ ‡={target_name}")
            
        return False

def test_json_parsing():
    """æµ‹è¯•JSONè§£æå‡½æ•°"""
    print("\nğŸ§ª æµ‹è¯•JSONè§£æåŠŸèƒ½...")
    
    llm_client = LLMClient()
    
    # æµ‹è¯•ç”¨ä¾‹1: æ ‡å‡†JSONæ•°ç»„
    test1 = '[{"task_id":0,"offload_ratio":0.8,"target_node":1},{"task_id":1,"offload_ratio":1.0,"target_node":3}]'
    result1 = llm_client._extract_json_from_text(test1)
    print(f"æµ‹è¯•1 - æ ‡å‡†JSON: {'âœ… æˆåŠŸ' if len(result1) == 2 else 'âŒ å¤±è´¥'}")
    
    # æµ‹è¯•ç”¨ä¾‹2: åŒ…å«æ€è€ƒè¿‡ç¨‹çš„æ–‡æœ¬
    test2 = """æˆ‘éœ€è¦åˆ†æè¿™äº›ä»»åŠ¡...
    
    [{"task_id": 0, "offload_ratio": 0.7, "target_node": 2}, {"task_id": 1, "offload_ratio": 1.0, "target_node": 3}]
    
    è¿™æ ·çš„ç­–ç•¥æ¯”è¾ƒåˆç†ã€‚"""
    result2 = llm_client._extract_json_from_text(test2)
    print(f"æµ‹è¯•2 - åŒ…å«æ–‡æœ¬: {'âœ… æˆåŠŸ' if len(result2) == 2 else 'âŒ å¤±è´¥'}")
    
    # æµ‹è¯•ç”¨ä¾‹3: æ ¼å¼ä¸å®Œæ•´çš„æ–‡æœ¬
    test3 = """ä»»åŠ¡0: offload_ratio: 0.8, target_node: 1
    ä»»åŠ¡1: offload_ratio: 1.0, target_node: 3"""
    result3 = llm_client._extract_json_from_text(test3)
    print(f"æµ‹è¯•3 - æ ¼å¼ä¸å®Œæ•´: {'âœ… æˆåŠŸ' if len(result3) >= 1 else 'âŒ å¤±è´¥'}")
    
    print(f"âœ… JSONè§£ææµ‹è¯•å®Œæˆ")

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹LLMè§£æåŠŸèƒ½æµ‹è¯•")
    print("=" * 50)
    
    # æµ‹è¯•JSONè§£æåŠŸèƒ½
    test_json_parsing()
    
    # æµ‹è¯•å®Œæ•´çš„LLMäº¤äº’
    llm_success = test_llm_parsing()
    
    print("\n" + "=" * 50)
    if llm_success:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼LLMè¿æ¥å’Œè§£æåŠŸèƒ½æ­£å¸¸")
    else:
        print("âš ï¸ LLMè¿æ¥å¤±è´¥ï¼Œä½†æ¨¡æ‹Ÿç­–ç•¥åŠŸèƒ½æ­£å¸¸")
    print("ğŸ æµ‹è¯•å®Œæˆ") 