# llm_assistant/llm_client.py
import requests
import json
import time
import numpy as np
import re


class LLMClient:
    """LLMå®¢æˆ·ç«¯ç±» - é€‚é…ç®€åŒ–è®¾å¤‡æ¨¡å‹"""
    
    def __init__(self, api_key="", model_name="qwen3-14b", server_url="http://10.200.1.35:8888/v1/completions",
                 timeout_connect=120, timeout_read=300, use_mock=True, config=None):
        """
        åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        
        Args:
            api_key: APIå¯†é’¥
            model_name: æ¨¡å‹åç§°
            server_url: LLMæœåŠ¡å™¨URL
            timeout_connect: è¿æ¥è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            timeout_read: è¯»å–è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            use_mock: æ˜¯å¦åœ¨å¤±è´¥æ—¶ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼
            config: é…ç½®å­—å…¸ï¼Œç”¨äºè¯»å–max_tokensç­‰å‚æ•°
        """
        self.api_key = api_key
        self.model_name = model_name
        self.server_url = server_url
        self.timeout_connect = timeout_connect
        self.timeout_read = timeout_read
        self.use_mock = use_mock
        
        # ä»é…ç½®ä¸­è¯»å–LLMå‚æ•°
        if config and 'llm' in config:
            llm_config = config['llm']
            self.max_tokens = llm_config.get('max_tokens', 4096)
            self.temperature = llm_config.get('temperature', 0.3)
        else:
            # é»˜è®¤å€¼
            self.max_tokens = 4096
            self.temperature = 0.3
        
        print(f"âœ… LLMå®¢æˆ·ç«¯åˆå§‹åŒ–:")
        print(f"  æœåŠ¡å™¨: {self.server_url}")
        print(f"  æ¨¡å‹: {self.model_name}")
        print(f"  æœ€å¤§tokens: {self.max_tokens}")
        print(f"  æ¸©åº¦: {self.temperature}")
        print(f"  è¶…æ—¶è®¾ç½®: è¿æ¥{self.timeout_connect}s, è¯»å–{self.timeout_read}s")
        print(f"  æ¨¡æ‹Ÿæ¨¡å¼: {'å¯ç”¨' if self.use_mock else 'ç¦ç”¨'}")

    def query(self, prompt):
        """
        æŸ¥è¯¢LLMæœåŠ¡å™¨
        
        Args:
            prompt: è¾“å…¥æç¤ºè¯
            
        Returns:
            str: LLMè¿”å›çš„æ–‡æœ¬å“åº”
        """
        try:
            # æ„å»ºè¯·æ±‚å¤´
            headers = {
                "Content-Type": "application/json",
            }
            if self.api_key:
                headers["Authorization"] = f"Bearer {self.api_key}"
                
            # æ„å»ºè¯·æ±‚æ•°æ®
            data = {
                "model": self.model_name,
                "prompt": prompt,
                "max_tokens": self.max_tokens,    # ä½¿ç”¨é…ç½®ä¸­çš„å€¼
                "temperature": self.temperature,  # ä½¿ç”¨é…ç½®ä¸­çš„å€¼
                "stream": False      # ä¸ä½¿ç”¨æµå¼è¿”å›
            }
            
            print(f"\n=== å‘é€LLMè¯·æ±‚ ===")
            print(f"ç›®æ ‡æœåŠ¡å™¨: {self.server_url}")
            print(f"æ¨¡å‹åç§°: {self.model_name}")
            print(f"æç¤ºé•¿åº¦: {len(prompt)}å­—ç¬¦")
            print(f"æœ€å¤§tokens: {self.max_tokens}")
            print(f"æ¸©åº¦: {self.temperature}")
            print(f"è¿æ¥è¶…æ—¶: {self.timeout_connect}ç§’")
            print(f"è¯»å–è¶…æ—¶: {self.timeout_read}ç§’")
            print("å¼€å§‹å‘é€è¯·æ±‚ï¼Œè¯·è€å¿ƒç­‰å¾…LLMå“åº”...")
            
            # è®°å½•å¼€å§‹æ—¶é—´
            start_time = time.time()
            
            # å‘é€è¯·æ±‚å¹¶ç­‰å¾…å“åº”
            response = requests.post(
                self.server_url, 
                headers=headers, 
                json=data, 
                timeout=(self.timeout_connect, self.timeout_read)  # (è¿æ¥è¶…æ—¶, è¯»å–è¶…æ—¶)
            )
            
            # è®¡ç®—å“åº”æ—¶é—´
            response_time = time.time() - start_time
            print(f"è¯·æ±‚å®Œæˆï¼Œè€—æ—¶: {response_time:.2f}ç§’")
            print(f"å“åº”çŠ¶æ€ç : {response.status_code}")
            
            # æ£€æŸ¥å“åº”çŠ¶æ€
            if response.status_code != 200:
                error_text = response.text[:500] if response.text else "æ— é”™è¯¯ä¿¡æ¯"
                print(f"LLMæœåŠ¡è¿”å›é”™è¯¯çŠ¶æ€ç : {response.status_code}")
                print(f"é”™è¯¯å“åº”å†…å®¹: {error_text}")
                raise Exception(f"LLMæœåŠ¡è¿”å›é”™è¯¯: HTTP {response.status_code}")
            
            # è§£æå“åº”JSON
            try:
                response_data = response.json()
            except json.JSONDecodeError as e:
                print(f"å“åº”JSONè§£æå¤±è´¥: {e}")
                print(f"åŸå§‹å“åº”å†…å®¹: {response.text[:1000]}")
                raise Exception("LLMå“åº”æ ¼å¼ä¸æ˜¯æœ‰æ•ˆçš„JSON")
            
            print(f"\n=== LLMå“åº”è§£æ ===")
            print(f"å“åº”æ•°æ®å­—æ®µ: {list(response_data.keys())}")
            
            # æå–å“åº”æ–‡æœ¬
            response_text = None
            
            # é€‚åº”ä¸åŒAPIç»“æ„
            if "choices" in response_data and len(response_data["choices"]) > 0:
                # OpenAIé£æ ¼API
                response_text = response_data["choices"][0].get("text", "").strip()
                print("ä½¿ç”¨OpenAIé£æ ¼APIå“åº”æ ¼å¼")
            elif "response" in response_data:
                # è‡ªå®šä¹‰APIé£æ ¼1
                response_text = response_data["response"].strip()
                print("ä½¿ç”¨è‡ªå®šä¹‰APIå“åº”æ ¼å¼1")
            elif "output" in response_data:
                # è‡ªå®šä¹‰APIé£æ ¼2
                response_text = response_data["output"].strip()
                print("ä½¿ç”¨è‡ªå®šä¹‰APIå“åº”æ ¼å¼2")
            elif "completion" in response_data:
                # è‡ªå®šä¹‰APIé£æ ¼3
                response_text = response_data["completion"].strip()
                print("ä½¿ç”¨è‡ªå®šä¹‰APIå“åº”æ ¼å¼3")
            else:
                # å°è¯•è¿”å›æ•´ä¸ªJSONå­—ç¬¦ä¸²
                response_text = json.dumps(response_data)
                print("ä½¿ç”¨å®Œæ•´JSONä½œä¸ºå“åº”")
            
            if not response_text:
                raise Exception("LLMå“åº”ä¸ºç©ºæˆ–æ— æ³•æå–æœ‰æ•ˆå†…å®¹")
            
            print(f"\n=== LLMåŸå§‹å“åº”å†…å®¹ ===")
            print("=" * 60)
            print(response_text)
            print("=" * 60)
            print(f"å“åº”é•¿åº¦: {len(response_text)}å­—ç¬¦")
            
            # ä¿å­˜åŸå§‹å“åº”JSONå’Œè§£æåçš„æ–‡æœ¬
            self._save_raw_response_to_file(response_data)
            self._save_response_to_file(response_text, is_mock=False)
            
            return response_text
                
        except requests.exceptions.ConnectTimeout:
            error_msg = f"è¿æ¥è¶…æ—¶: æ— æ³•åœ¨{self.timeout_connect}ç§’å†…è¿æ¥åˆ°LLMæœåŠ¡å™¨ {self.server_url}"
            print(f"\nâŒ {error_msg}")
            raise Exception(error_msg)
        except requests.exceptions.ReadTimeout:
            error_msg = f"è¯»å–è¶…æ—¶: LLMæœåŠ¡å™¨åœ¨{self.timeout_read}ç§’å†…æœªè¿”å›å®Œæ•´å“åº”"
            print(f"\nâŒ {error_msg}")
            raise Exception(error_msg)
        except requests.exceptions.ConnectionError as e:
            error_msg = f"è¿æ¥é”™è¯¯: æ— æ³•å»ºç«‹ä¸LLMæœåŠ¡å™¨çš„è¿æ¥ - {str(e)}"
            print(f"\nâŒ {error_msg}")
            raise Exception(error_msg)
        except Exception as e:
            error_msg = f"LLMæŸ¥è¯¢å¤±è´¥: {str(e)}"
            print(f"\nâŒ {error_msg}")
            raise Exception(error_msg)

    def get_unload_strategy(self, env_state, device_info, edge_info, cloud_info, tasks_info=None):
        """è·å–å¸è½½ç­–ç•¥å»ºè®® - é€‚é…ç®€åŒ–è®¾å¤‡æ¨¡å‹"""
        # æ„å»ºæç¤º
        prompt = self._build_prompt(env_state, device_info, edge_info, cloud_info, tasks_info)
        
        # ä¿å­˜æç¤ºåˆ°æ–‡ä»¶ä»¥ä¾¿è°ƒè¯•
        with open("last_prompt.txt", "w", encoding="utf-8") as f:
            f.write(prompt)
        print(f"å·²ä¿å­˜æç¤ºåˆ° last_prompt.txt (é•¿åº¦: {len(prompt)}å­—ç¬¦)")
        
        print("\nğŸ“¤ å‘é€å¸è½½ç­–ç•¥è¯·æ±‚åˆ°LLM...")
        # æŸ¥è¯¢LLM
        try:
            response = self.query(prompt)
            
            print(f"\nâœ… LLMå“åº”è·å–æˆåŠŸ!")
            
            # ä¿å­˜å“åº”åˆ°æ–‡ä»¶ä»¥ä¾¿è°ƒè¯•
            with open("last_response.txt", "w", encoding="utf-8") as f:
                f.write(response)
            print(f"å·²ä¿å­˜LLMå“åº”åˆ° last_response.txt")
            
            print(f"\nğŸ“‹ å¼€å§‹è§£æLLMå“åº”...")
            # è§£æå“åº”
            strategies = self._extract_json_from_text(response)
            
            if not strategies:
                print("âš ï¸ æ— æ³•è§£æLLMå“åº”ä¸­çš„æœ‰æ•ˆç­–ç•¥ï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥")
                strategies = self._generate_default_strategies(len(device_info))
            else:
                print(f"âœ… æˆåŠŸè§£æLLMç­–ç•¥: {len(strategies)}ä¸ªä»»åŠ¡çš„å¸è½½å†³ç­–")
                # æ‰“å°è§£æåˆ°çš„ç­–ç•¥æ¦‚è¦
                for i, strategy in enumerate(strategies):
                    task_id = strategy.get('task_id', i)
                    offload_ratio = strategy.get('offload_ratio', 0.0)
                    target_node = strategy.get('target_node', 0)
                    print(f"  ä»»åŠ¡{task_id}: å¸è½½æ¯”ä¾‹={offload_ratio:.2f}, ç›®æ ‡èŠ‚ç‚¹={target_node}")
            
            return strategies
            
        except Exception as e:
            print(f"\nâŒ LLMæœåŠ¡è°ƒç”¨å¤±è´¥: {e}")
            if self.use_mock:
                print("ğŸ”„ å¯ç”¨æ¨¡æ‹Ÿæ¨¡å¼ï¼Œä½¿ç”¨è§„åˆ™ç”Ÿæˆç­–ç•¥ç»§ç»­è®­ç»ƒ...")
                # ç”Ÿæˆæ¨¡æ‹Ÿç­–ç•¥
                return self._generate_mock_strategies(env_state, device_info, edge_info, cloud_info)
            else:
                print("ğŸ”„ è¿”å›é»˜è®¤ç­–ç•¥...")
                return self._generate_default_strategies(len(device_info))

    def _generate_mock_strategies(self, env_state, device_info, edge_info, cloud_info):
        """åœ¨LLMæœåŠ¡ä¸å¯ç”¨æ—¶ç”Ÿæˆæ™ºèƒ½æ¨¡æ‹Ÿç­–ç•¥
        
        åŸºäºä»»åŠ¡ç‰¹å¾å’Œèµ„æºçŠ¶æ€ç”Ÿæˆåˆç†çš„å¸è½½ç­–ç•¥
        """
        print("ğŸ¤– ç”Ÿæˆæ™ºèƒ½æ¨¡æ‹Ÿå¸è½½ç­–ç•¥...")
        strategies = []
        
        # ç®€å•è§£æç¯å¢ƒçŠ¶æ€
        try:
            env_state_array = np.array(env_state)
            device_states = env_state_array[:len(device_info) * 4].reshape(len(device_info), 4)
            edge_states = env_state_array[len(device_info) * 4: len(device_info) * 4 + len(edge_info) * 3].reshape(len(edge_info), 3)
            cloud_states = env_state_array[len(device_info) * 4 + len(edge_info) * 3:len(device_info) * 4 + len(edge_info) * 3 + len(cloud_info) * 3].reshape(len(cloud_info), 3)
            task_states = env_state_array[len(device_info) * 4 + len(edge_info) * 3 + len(cloud_info) * 3:].reshape(len(device_info), 3)
        except:
            # å¼‚å¸¸æƒ…å†µä¸‹ä½¿ç”¨é»˜è®¤ç­–ç•¥
            print("âš ï¸ ç¯å¢ƒçŠ¶æ€è§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥")
            return self._generate_default_strategies(len(device_info))
        
        print(f"ğŸ“Š åˆ†æ{len(device_info)}ä¸ªä»»åŠ¡çš„ç‰¹å¾...")
        
        # æ ¹æ®ä»»åŠ¡ç‰¹å¾å’Œèµ„æºçŠ¶æ€ç”Ÿæˆæ™ºèƒ½ç­–ç•¥
        for i in range(len(device_info)):
            device_battery = device_states[i][2] if len(device_states[i]) > 2 else 1.0  # ç”µæ± çŠ¶æ€
            task_computation = task_states[i][0]    # è®¡ç®—é‡ (MI)
            task_data_size = task_states[i][1]      # æ•°æ®é‡ (MB)
            task_deadline = task_states[i][2]       # æˆªæ­¢æ—¶é—´ (ç§’)
            
            # è®¡ç®—å¤„ç†æ—¶é—´ä¼°ç®—
            local_time = task_computation / 2.0     # æœ¬åœ°å¤„ç†æ—¶é—´ (2GHz)
            edge_time = task_computation / 8.0      # è¾¹ç¼˜å¤„ç†æ—¶é—´ (8GHz)
            cloud_time = task_computation / 32.0    # äº‘ç«¯å¤„ç†æ—¶é—´ (32GHz)
            
            # ç®€å•çš„ä¼ è¾“æ—¶é—´ä¼°ç®— (å‡è®¾ç½‘ç»œå¸¦å®½)
            edge_transmission = task_data_size / 50.0   # å‡è®¾50MB/såˆ°è¾¹ç¼˜
            cloud_transmission = task_data_size / 25.0  # å‡è®¾25MB/såˆ°äº‘ç«¯
            
            # å†³ç­–é€»è¾‘
            offload_ratio = 0.0
            target_node = 0  # é»˜è®¤æœ¬åœ°
            
            # å¦‚æœæœ¬åœ°å¤„ç†æ—¶é—´è¶…è¿‡æˆªæ­¢æ—¶é—´ï¼Œå¿…é¡»å¸è½½
            if local_time > task_deadline:
                # æ¯”è¾ƒè¾¹ç¼˜å’Œäº‘ç«¯çš„æ€»æ—¶é—´
                edge_total_time = edge_time + edge_transmission
                cloud_total_time = cloud_time + cloud_transmission
                
                if edge_total_time <= task_deadline and edge_total_time <= cloud_total_time:
                    # å¸è½½åˆ°è¾¹ç¼˜æœåŠ¡å™¨
                    offload_ratio = 1.0
                    target_node = (i % len(edge_info)) + 1  # è½®è¯¢åˆ†é…è¾¹ç¼˜æœåŠ¡å™¨
                    
                elif cloud_total_time <= task_deadline:
                    # å¸è½½åˆ°äº‘ç«¯
                    offload_ratio = 1.0
                    target_node = len(edge_info) + 1  # äº‘ç«¯èŠ‚ç‚¹
                    
                else:
                    # å³ä½¿è¶…æ—¶ä¹Ÿé€‰æ‹©æœ€å¿«çš„é€‰é¡¹
                    if cloud_total_time < edge_total_time:
                        offload_ratio = 1.0
                        target_node = len(edge_info) + 1  # äº‘ç«¯
                    else:
                        offload_ratio = 1.0
                        target_node = (i % len(edge_info)) + 1  # è¾¹ç¼˜
                        
            else:
                # æœ¬åœ°å¯ä»¥å®Œæˆï¼Œä½†è€ƒè™‘ç”µæ± çŠ¶æ€
                if device_battery < 0.2:  # ç”µæ± ä½äº20%
                    # éƒ¨åˆ†å¸è½½ä»¥èŠ‚çœç”µé‡
                    if task_computation > 500:  # è®¡ç®—å¯†é›†å‹ä»»åŠ¡
                        offload_ratio = 0.7
                        # é€‰æ‹©æ›´é«˜æ•ˆçš„ç›®æ ‡
                        if cloud_time + cloud_transmission < edge_time + edge_transmission:
                            target_node = len(edge_info) + 1  # äº‘ç«¯
                        else:
                            target_node = (i % len(edge_info)) + 1  # è¾¹ç¼˜
                    else:
                        offload_ratio = 0.3
                        target_node = (i % len(edge_info)) + 1  # è¾¹ç¼˜
                else:
                    # ç”µæ± å……è¶³ï¼Œå¯ä»¥è€ƒè™‘æœ¬åœ°å¤„ç†æˆ–è½»åº¦å¸è½½
                    if task_computation > 800:  # éå¸¸è®¡ç®—å¯†é›†
                        offload_ratio = 0.5
                        target_node = len(edge_info) + 1  # äº‘ç«¯
                    elif task_computation > 400:  # ä¸­ç­‰è®¡ç®—é‡
                        offload_ratio = 0.3
                        target_node = (i % len(edge_info)) + 1  # è¾¹ç¼˜
                    # else: ä¿æŒæœ¬åœ°å¤„ç† (offload_ratio = 0.0, target_node = 0)
            
            # ç¡®ä¿å€¼åœ¨åˆæ³•èŒƒå›´å†…
            offload_ratio = max(0.0, min(1.0, offload_ratio))
            target_node = max(0, min(len(edge_info) + len(cloud_info), target_node))
            
            strategy = {
                "task_id": i,
                "offload_ratio": round(offload_ratio, 2),
                "target_node": target_node
            }
            strategies.append(strategy)
            
            # æ‰“å°å†³ç­–ç†ç”±
            target_name = "æœ¬åœ°"
            if target_node >= 1 and target_node <= len(edge_info):
                target_name = f"è¾¹ç¼˜æœåŠ¡å™¨{target_node-1}"
            elif target_node > len(edge_info):
                target_name = "äº‘ç«¯"
                
            print(f"  ä»»åŠ¡{i}: è®¡ç®—é‡{task_computation:.0f}MI, æˆªæ­¢{task_deadline:.1f}s -> "
                  f"å¸è½½{offload_ratio:.2f}åˆ°{target_name}")
        
        print(f"âœ… ç”Ÿæˆ{len(strategies)}ä¸ªæ™ºèƒ½æ¨¡æ‹Ÿç­–ç•¥")
        return strategies

    def _extract_json_from_text(self, text):
        """ä»æ–‡æœ¬ä¸­æå–JSONå†…å®¹ï¼Œä½¿ç”¨å¤šç§ç­–ç•¥ç¡®ä¿è§£ææˆåŠŸ"""
        print(f"å¼€å§‹è§£æLLMå“åº”æ–‡æœ¬ï¼Œé•¿åº¦: {len(text)}å­—ç¬¦")
        
        # é¢„å¤„ç†ï¼šç§»é™¤å¯èƒ½çš„markdownæ ‡è®°å’Œå¤šä½™çš„ç©ºç™½
        text = text.strip()
        
        # ç­–ç•¥1: å°è¯•ç›´æ¥è§£ææ•´ä¸ªæ–‡æœ¬ä¸ºJSON
        try:
            result = json.loads(text)
            if isinstance(result, list):
                print("âœ… ç­–ç•¥1æˆåŠŸï¼šç›´æ¥è§£æä¸ºJSONæ•°ç»„")
                return result
            elif isinstance(result, dict):
                print("âœ… ç­–ç•¥1æˆåŠŸï¼šè§£æä¸ºå•ä¸ªJSONå¯¹è±¡ï¼Œè½¬æ¢ä¸ºæ•°ç»„")
                return [result]
        except:
            pass
        
        # ç­–ç•¥2: æŸ¥æ‰¾å¹¶æå– [...]  æ ¼å¼çš„JSONæ•°ç»„
        array_pattern = r'\[[\s\S]*?\]'
        array_matches = re.findall(array_pattern, text)
        
        for match in array_matches:
            try:
                result = json.loads(match)
                if isinstance(result, list):
                    print("âœ… ç­–ç•¥2æˆåŠŸï¼šæå–JSONæ•°ç»„æ ¼å¼")
                    return result
            except:
                continue
        
        # ç­–ç•¥3: æŸ¥æ‰¾å¹¶æå– {...} æ ¼å¼çš„JSONå¯¹è±¡
        object_pattern = r'\{[\s\S]*?\}'
        object_matches = re.findall(object_pattern, text)
        
        valid_objects = []
        for match in object_matches:
            try:
                obj = json.loads(match)
                if isinstance(obj, dict) and 'task_id' in obj:
                    valid_objects.append(obj)
            except:
                continue
        
        if valid_objects:
            print(f"âœ… ç­–ç•¥3æˆåŠŸï¼šæå–åˆ°{len(valid_objects)}ä¸ªJSONå¯¹è±¡")
            return valid_objects
        
        # ç­–ç•¥4: ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ç›´æ¥æå–å…³é”®ä¿¡æ¯
        pattern = r'task_id["\s]*:[\s]*(\d+)[\s\S]*?offload_ratio["\s]*:[\s]*([\d.]+)[\s\S]*?target_node["\s]*:[\s]*(\d+)'
        matches = re.findall(pattern, text, re.IGNORECASE)
        
        if matches:
            result = []
            for match in matches:
                try:
                    task_id = int(match[0])
                    offload_ratio = float(match[1])
                    target_node = int(match[2])
                    result.append({
                        "task_id": task_id,
                        "offload_ratio": offload_ratio,
                        "target_node": target_node
                    })
                except:
                    continue
            
            if result:
                print(f"âœ… ç­–ç•¥4æˆåŠŸï¼šé€šè¿‡æ­£åˆ™è¡¨è¾¾å¼æå–{len(result)}ä¸ªç­–ç•¥")
                return result
        
        # ç­–ç•¥5: æŸ¥æ‰¾æ•°å­—åºåˆ—ï¼Œå°è¯•æ„å»ºç­–ç•¥
        lines = text.split('\n')
        result = []
        
        for line in lines:
            # æŸ¥æ‰¾åŒ…å«ä»»åŠ¡ä¿¡æ¯çš„è¡Œ
            task_match = re.search(r'ä»»åŠ¡(\d+).*?(\d+\.?\d*).*?(\d+)', line)
            if task_match:
                try:
                    task_id = int(task_match.group(1))
                    offload_ratio = float(task_match.group(2)) if '.' in task_match.group(2) else float(task_match.group(2)) / 10
                    target_node = int(task_match.group(3))
                    
                    # ç¡®ä¿å€¼åœ¨åˆç†èŒƒå›´å†…
                    if 0 <= offload_ratio <= 1 and 0 <= target_node <= 3:
                        result.append({
                            "task_id": task_id,
                            "offload_ratio": offload_ratio,
                            "target_node": target_node
                        })
                except:
                    continue
        
        if result:
            print(f"âœ… ç­–ç•¥5æˆåŠŸï¼šä»æ–‡æœ¬è¡Œä¸­æå–{len(result)}ä¸ªç­–ç•¥")
            return result
        
        print("âŒ æ‰€æœ‰è§£æç­–ç•¥å‡å¤±è´¥")
        return []
    
    def _generate_default_strategies(self, num_tasks):
        """ç”Ÿæˆé»˜è®¤å¸è½½ç­–ç•¥"""
        return [
            {
                "task_id": i,
                "offload_ratio": 0.0,
                "target_node": 0
            }
            for i in range(num_tasks)
        ]

    def _build_prompt(self, env_state, device_info, edge_info, cloud_info, tasks_info=None):
        """æ„å»ºLLMæç¤ºè¯ - é€‚é…ç®€åŒ–è®¾å¤‡æ¨¡å‹"""
        try:
            # å¯¼å…¥æ–°çš„æç¤ºè¯æ„å»ºå™¨
            from llm_assistant.prompt_builder import PromptBuilder
            
            # å¦‚æœæ²¡æœ‰æä¾›ä»»åŠ¡ä¿¡æ¯ï¼Œä»device_infoæ¨æ–­
            if tasks_info is None:
                tasks_info = []
                for i in range(len(device_info)):
                    tasks_info.append({
                        'task_id': f'task_{i}',
                        'device_id': i,
                        'task_type': 'medium',
                        'data_size': 25.0,
                        'cpu_cycles': 5e9,
                        'deadline': 30.0,
                        'remaining_time': 25.0
                    })
            
            # ä½¿ç”¨æ–°çš„æç¤ºè¯æ„å»ºå™¨
            prompt = PromptBuilder.build_offloading_strategy_prompt(
                env_state, device_info, edge_info, cloud_info, tasks_info
            )
            print("âœ… ä½¿ç”¨ç®€åŒ–è®¾å¤‡æ¨¡å‹çš„æç¤ºè¯æ„å»ºå™¨")
            return prompt
            
        except ImportError as e:
            print(f"âš ï¸ æç¤ºè¯æ„å»ºå™¨å¯¼å…¥å¤±è´¥: {e}")
            # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–çš„å¤‡ç”¨æç¤ºæ¨¡æ¿
            return self._build_fallback_prompt(device_info, edge_info, cloud_info)
    
    def _build_fallback_prompt(self, device_info, edge_info, cloud_info):
        """å¤‡ç”¨ç®€åŒ–æç¤ºæ¨¡æ¿ - é€‚é…ç®€åŒ–è®¾å¤‡æ¨¡å‹"""
        print("ğŸ”„ ä½¿ç”¨å¤‡ç”¨ç®€åŒ–æç¤ºæ¨¡æ¿")
        
        prompt = f"""ä½ æ˜¯äº‘è¾¹ç«¯è®¡ç®—å¸è½½ä¸“å®¶ã€‚ç³»ç»Ÿé‡‡ç”¨ç®€åŒ–è®¾å¤‡æ¨¡å‹ï¼š

**è®¾å¤‡çŠ¶æ€ï¼ˆç®€åŒ–ç‰ˆï¼‰**:
- UEè®¾å¤‡({len(device_info)}ä¸ª): CPUé¢‘ç‡ + ç”µæ±  + ä»»åŠ¡è´Ÿè½½
- ESæœåŠ¡å™¨({len(edge_info)}ä¸ª): CPUé¢‘ç‡ + ä»»åŠ¡è´Ÿè½½
- CSæœåŠ¡å™¨({len(cloud_info)}ä¸ª): CPUé¢‘ç‡ï¼ˆèµ„æºæ— é™ï¼‰

**å½“å‰çŠ¶æ€**:
"""
        
        # UEè®¾å¤‡çŠ¶æ€
        for i, device in enumerate(device_info):
            battery_pct = device.get('battery_percentage', 0.5) * 100
            cpu_freq = device.get('cpu_frequency', 0.8)
            task_load = device.get('task_load', 0.0)
            prompt += f"UE{i}: {cpu_freq:.1f}GHz, ç”µæ± {battery_pct:.0f}%, è´Ÿè½½{task_load:.1f}s\n"
        
        # ESæœåŠ¡å™¨çŠ¶æ€  
        for i, server in enumerate(edge_info):
            cpu_freq = server.get('cpu_frequency', 8)
            task_load = server.get('task_load', 0.0)
            prompt += f"ES{i}: {cpu_freq}GHz, è´Ÿè½½{task_load:.1f}s\n"
            
        prompt += f"""
**é€šä¿¡å»¶è¿Ÿå·®å¼‚**:
- è¾¹ç¼˜å¸è½½: ä½å»¶è¿Ÿ(1Gbpsç›´è¿)
- äº‘ç«¯å¸è½½: é«˜å»¶è¿Ÿ(éœ€è¦ä¸­è½¬)

**ä»»åŠ¡åˆ†å‰²ç­–ç•¥**:
æ¯ä¸ªUEè®¾å¤‡éœ€è¦å†³ç­–: [Î±1, Î±2, Î±3, edge_id]
- Î±1: æœ¬åœ°æ‰§è¡Œæ¯”ä¾‹
- Î±2: è¾¹ç¼˜æ‰§è¡Œæ¯”ä¾‹  
- Î±3: äº‘ç«¯æ‰§è¡Œæ¯”ä¾‹
- edge_id: ç›®æ ‡è¾¹ç¼˜æœåŠ¡å™¨(0-{len(edge_info)-1})

è¦æ±‚ç›´æ¥è¿”å›JSONæ ¼å¼çš„ç­–ç•¥æ•°ç»„:

[
  {{"device_id": 0, "local_ratio": 0.3, "edge_ratio": 0.5, "cloud_ratio": 0.2, "target_edge": 1}},
  {{"device_id": 1, "local_ratio": 0.0, "edge_ratio": 0.8, "cloud_ratio": 0.2, "target_edge": 0}}
]

JSON:"""
        
        return prompt
    
    def _save_prompt_to_file(self, prompt):
        """ä¿å­˜æç¤ºè¯åˆ°æ–‡ä»¶"""
        try:
            import datetime
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # ä¿å­˜æœ€æ–°çš„æç¤ºè¯
            with open("last_prompt.txt", "w", encoding="utf-8") as f:
                f.write("=" * 60 + "\n")
                f.write("å‘é€ç»™LLMçš„æç¤ºè¯å†…å®¹\n")
                f.write("=" * 60 + "\n")
                f.write(prompt)
                f.write("\n" + "=" * 60 + "\n")
            
            # ä¿å­˜å¸¦æ—¶é—´æˆ³çš„æç¤ºè¯
            with open(f"prompt_{timestamp}.txt", "w", encoding="utf-8") as f:
                f.write("=" * 60 + "\n")
                f.write(f"å‘é€ç»™LLMçš„æç¤ºè¯å†…å®¹ - {timestamp}\n")
                f.write("=" * 60 + "\n")
                f.write(prompt)
                f.write("\n" + "=" * 60 + "\n")
            
            print(f"âœ… æç¤ºè¯å·²ä¿å­˜åˆ°: last_prompt.txt å’Œ prompt_{timestamp}.txt")
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜æç¤ºè¯å¤±è´¥: {e}")
    
    def _save_response_to_file(self, response_text, is_mock=False):
        """ä¿å­˜LLMå“åº”æ–‡æœ¬åˆ°æ–‡ä»¶"""
        try:
            import datetime
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            mode_label = "æ¨¡æ‹Ÿæ¨¡å¼" if is_mock else "LLMæœåŠ¡å™¨"
            
            # ä¿å­˜æœ€æ–°çš„å“åº”
            with open("last_response.txt", "w", encoding="utf-8") as f:
                f.write("=" * 60 + "\n")
                f.write(f"LLMè¿”å›çš„åŸå§‹å†…å®¹ ({mode_label})\n")
                f.write("=" * 60 + "\n")
                f.write(response_text)
                f.write("\n" + "=" * 60 + "\n")
            
            # ä¿å­˜å¸¦æ—¶é—´æˆ³çš„å“åº”
            with open(f"response_{timestamp}.txt", "w", encoding="utf-8") as f:
                f.write("=" * 60 + "\n")
                f.write(f"LLMè¿”å›çš„åŸå§‹å†…å®¹ ({mode_label}) - {timestamp}\n")
                f.write("=" * 60 + "\n")
                f.write(response_text)
                f.write("\n" + "=" * 60 + "\n")
            
            print(f"âœ… LLMå“åº”å·²ä¿å­˜åˆ°: last_response.txt å’Œ response_{timestamp}.txt")
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜LLMå“åº”å¤±è´¥: {e}")
    
    def _save_raw_response_to_file(self, raw_json):
        """ä¿å­˜LLMåŸå§‹JSONå“åº”åˆ°æ–‡ä»¶"""
        try:
            import datetime
            import json
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # ä¿å­˜æœ€æ–°çš„åŸå§‹å“åº”
            with open("last_raw_response.json", "w", encoding="utf-8") as f:
                json.dump(raw_json, f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜å¸¦æ—¶é—´æˆ³çš„åŸå§‹å“åº”
            with open(f"raw_response_{timestamp}.json", "w", encoding="utf-8") as f:
                json.dump(raw_json, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… LLMåŸå§‹å“åº”å·²ä¿å­˜åˆ°: last_raw_response.json å’Œ raw_response_{timestamp}.json")
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜LLMåŸå§‹å“åº”å¤±è´¥: {e}")