#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLM+MADDPG äº‘è¾¹ç«¯è®¡ç®—å¸è½½ç³»ç»Ÿ
ç»Ÿä¸€å…¥å£æ–‡ä»¶ - æ”¯æŒå®Œæ•´çš„è®­ç»ƒã€æµ‹è¯•ã€å¯¹æ¯”åŠŸèƒ½
"""

import os
import sys
import json
import time
import yaml
import random
import argparse
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime
from typing import Dict, List, Tuple
import torch

# å¯¼å…¥è®­ç»ƒå’Œæµ‹è¯•æ¨¡å—
from experiments.train_llm_maddpg_complete import train_llm_maddpg_complete
from experiments.train_maddpg import train_maddpg
from experiments.train_llm import train_llm
from experiments.test_llm_maddpg import test_llm_maddpg
from experiments.test_maddpg import test_maddpg
from experiments.test_llm import test_llm
from utils.config import load_config


def set_seed(seed):
    """è®¾ç½®éšæœºç§å­ç¡®ä¿å¯å¤ç°æ€§"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def print_banner():
    """æ‰“å°é¡¹ç›®å¯åŠ¨æ¨ªå¹…"""
    banner = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                  LLM+MADDPG äº‘è¾¹ç«¯è®¡ç®—å¸è½½ç³»ç»Ÿ                  â•‘
â•‘                        ç»Ÿä¸€è®­ç»ƒæµ‹è¯•å¹³å°                         â•‘
â•‘                                                              â•‘
â•‘  æ”¯æŒç®—æ³•ï¼š                                                    â•‘
â•‘  â€¢ LLM+MADDPGï¼ˆå®Œæ•´ç‰ˆï¼‰- æ¯stepå’¨è¯¢LLM + çŸ¥è¯†è’¸é¦                 â•‘
â•‘  â€¢ çº¯MADDPG - å¤šæ™ºèƒ½ä½“æ·±åº¦ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦                          â•‘
â•‘  â€¢ çº¯LLM - å¤§è¯­è¨€æ¨¡å‹ç›´æ¥å†³ç­–                                    â•‘
â•‘                                                              â•‘
â•‘  åŠŸèƒ½ï¼šè®­ç»ƒ â†’ æµ‹è¯• â†’ æ€§èƒ½å¯¹æ¯” â†’ ç»“æœå¯è§†åŒ–                         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """
    print(banner)


def create_result_directories(base_dir="results"):
    """åˆ›å»ºç»“æœä¿å­˜ç›®å½•"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    experiment_dir = f"{base_dir}/experiment_{timestamp}"
    
    directories = [
        experiment_dir,
        f"{experiment_dir}/llm_maddpg",
        f"{experiment_dir}/pure_maddpg", 
        f"{experiment_dir}/pure_llm",
        f"{experiment_dir}/comparison",
        f"{experiment_dir}/logs",
        f"{experiment_dir}/plots"
    ]
    
    for directory in directories:
        os.makedirs(directory, exist_ok=True)
    
    return experiment_dir


def train_llm_maddpg_complete(config, experiment_dir):
    """è®­ç»ƒLLM+MADDPGå®Œæ•´ç‰ˆ"""
    print("\n" + "="*80)
    print("ğŸš€ è®­ç»ƒ LLM+MADDPGï¼ˆå®Œæ•´ç‰ˆï¼‰")
    print("="*80)
    
    try:
        from experiments.train_llm_maddpg_complete import train_llm_maddpg_complete
        
        # è®­ç»ƒ
        start_time = time.time()
        results = train_llm_maddpg_complete("config.yaml")
        training_time = time.time() - start_time
        
        # ä¿å­˜ç»“æœ
        results['training_time'] = training_time
        results['algorithm'] = 'LLM+MADDPG'
        
        result_file = f"{experiment_dir}/llm_maddpg/training_results.json"
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… LLM+MADDPGè®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {training_time:.2f}ç§’")
        print(f"ğŸ’¾ ç»“æœä¿å­˜è‡³: {result_file}")
        
        return results
        
    except Exception as e:
        print(f"âŒ LLM+MADDPGè®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return {}


def train_pure_algorithms(config, experiment_dir):
    """è®­ç»ƒçº¯MADDPGå’Œçº¯LLMç®—æ³•"""
    results = {}
    
    # è®­ç»ƒçº¯MADDPG
    print("\n" + "="*80)
    print("ğŸ”¥ è®­ç»ƒ çº¯MADDPG")
    print("="*80)
    
    try:
        start_time = time.time()
        maddpg_result = train_maddpg(config)
        training_time = time.time() - start_time
        
        results['pure_maddpg'] = {
            'training_time': training_time,
            'algorithm': 'çº¯MADDPG',
            'result': maddpg_result
        }
        
        result_file = f"{experiment_dir}/pure_maddpg/training_results.json"
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(results['pure_maddpg'], f, indent=2, ensure_ascii=False)
        
        print(f"âœ… çº¯MADDPGè®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {training_time:.2f}ç§’")
        
    except Exception as e:
        print(f"âŒ çº¯MADDPGè®­ç»ƒå¤±è´¥: {e}")
        results['pure_maddpg'] = {'error': str(e)}
    
    # è®­ç»ƒçº¯LLM
    print("\n" + "="*80)
    print("ğŸ§  è®­ç»ƒ çº¯LLM")
    print("="*80)
    
    try:
        start_time = time.time()
        llm_result = train_llm(config)
        training_time = time.time() - start_time
        
        results['pure_llm'] = {
            'training_time': training_time,
            'algorithm': 'çº¯LLM',
            'result': llm_result
        }
        
        result_file = f"{experiment_dir}/pure_llm/training_results.json"
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(results['pure_llm'], f, indent=2, ensure_ascii=False)
        
        print(f"âœ… çº¯LLMè®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {training_time:.2f}ç§’")
        
    except Exception as e:
        print(f"âŒ çº¯LLMè®­ç»ƒå¤±è´¥: {e}")
        results['pure_llm'] = {'error': str(e)}
    
    return results


def run_algorithm_tests(config, experiment_dir):
    """è¿è¡Œç®—æ³•æµ‹è¯•"""
    print("\n" + "="*80)
    print("ğŸ§ª ç®—æ³•æ€§èƒ½æµ‹è¯•")
    print("="*80)
    
    test_results = {}
    
    # æµ‹è¯•LLM+MADDPG
    try:
        print("ğŸ”¬ æµ‹è¯• LLM+MADDPG...")
        llm_maddpg_metrics = test_llm_maddpg(f"{experiment_dir}/llm_maddpg", config)
        if isinstance(llm_maddpg_metrics, tuple) and len(llm_maddpg_metrics) == 3:
            energy, util, delay = llm_maddpg_metrics
            test_results['llm_maddpg'] = {
                'energy': np.mean(energy) if energy else 0,
                'utilization': np.mean(util) if util else 0,
                'delay': np.mean(delay) if delay else 0,
                'energy_std': np.std(energy) if energy else 0,
                'utilization_std': np.std(util) if util else 0,
                'delay_std': np.std(delay) if delay else 0,
            }
            print(f"  âœ… èƒ½è€—: {test_results['llm_maddpg']['energy']:.4f}, æ—¶å»¶: {test_results['llm_maddpg']['delay']:.4f}")
    except Exception as e:
        print(f"  âŒ LLM+MADDPGæµ‹è¯•å¤±è´¥: {e}")
        test_results['llm_maddpg'] = {'error': str(e)}
    
    # æµ‹è¯•çº¯MADDPG
    try:
        print("ğŸ”¬ æµ‹è¯• çº¯MADDPG...")
        maddpg_metrics = test_maddpg(f"{experiment_dir}/pure_maddpg", config)
        if isinstance(maddpg_metrics, tuple) and len(maddpg_metrics) == 3:
            energy, util, delay = maddpg_metrics
            test_results['pure_maddpg'] = {
                'energy': np.mean(energy) if energy else 0,
                'utilization': np.mean(util) if util else 0,
                'delay': np.mean(delay) if delay else 0,
                'energy_std': np.std(energy) if energy else 0,
                'utilization_std': np.std(util) if util else 0,
                'delay_std': np.std(delay) if delay else 0,
            }
            print(f"  âœ… èƒ½è€—: {test_results['pure_maddpg']['energy']:.4f}, æ—¶å»¶: {test_results['pure_maddpg']['delay']:.4f}")
    except Exception as e:
        print(f"  âŒ çº¯MADDPGæµ‹è¯•å¤±è´¥: {e}")
        test_results['pure_maddpg'] = {'error': str(e)}
    
    # æµ‹è¯•çº¯LLM
    try:
        print("ğŸ”¬ æµ‹è¯• çº¯LLM...")
        llm_metrics = test_llm(f"{experiment_dir}/pure_llm", config)
        if isinstance(llm_metrics, tuple) and len(llm_metrics) == 3:
            energy, util, delay = llm_metrics
            test_results['pure_llm'] = {
                'energy': np.mean(energy) if energy else 0,
                'utilization': np.mean(util) if util else 0,
                'delay': np.mean(delay) if delay else 0,
                'energy_std': np.std(energy) if energy else 0,
                'utilization_std': np.std(util) if util else 0,
                'delay_std': np.std(delay) if delay else 0,
            }
            print(f"  âœ… èƒ½è€—: {test_results['pure_llm']['energy']:.4f}, æ—¶å»¶: {test_results['pure_llm']['delay']:.4f}")
    except Exception as e:
        print(f"  âŒ çº¯LLMæµ‹è¯•å¤±è´¥: {e}")
        test_results['pure_llm'] = {'error': str(e)}
    
    # ä¿å­˜æµ‹è¯•ç»“æœ
    test_file = f"{experiment_dir}/comparison/test_results.json"
    with open(test_file, 'w', encoding='utf-8') as f:
        json.dump(test_results, f, indent=2, ensure_ascii=False)
    
    return test_results


def generate_comparison_report(training_results, test_results, experiment_dir):
    """ç”Ÿæˆè¯¦ç»†çš„å¯¹æ¯”æŠ¥å‘Š"""
    print("\n" + "="*80)
    print("ğŸ“Š ç”Ÿæˆç®—æ³•å¯¹æ¯”æŠ¥å‘Š")
    print("="*80)
    
    # æ•´åˆæ‰€æœ‰ç»“æœ
    comparison_data = []
    
    # LLM+MADDPGç»“æœ
    if training_results and test_results.get('llm_maddpg'):
        llm_maddpg_data = {
            'algorithm': 'LLM+MADDPG',
            'training_time': training_results.get('training_time', 0),
            'avg_reward': np.mean(training_results.get('episode_rewards', [0])[-50:]),
            'avg_latency': np.mean(training_results.get('episode_latencies', [0])[-50:]),
            'avg_energy': np.mean(training_results.get('episode_energies', [0])[-50:]),
            'completion_rate': np.mean(training_results.get('episode_completion_rates', [0])[-50:]),
            'test_energy': test_results['llm_maddpg'].get('energy', 0),
            'test_delay': test_results['llm_maddpg'].get('delay', 0),
            'test_utilization': test_results['llm_maddpg'].get('utilization', 0),
        }
        comparison_data.append(llm_maddpg_data)
    
    # çº¯MADDPGå’Œçº¯LLMç»“æœï¼ˆç®€åŒ–å¤„ç†ï¼‰
    for algo_name in ['pure_maddpg', 'pure_llm']:
        if test_results.get(algo_name) and 'error' not in test_results[algo_name]:
            algo_data = {
                'algorithm': 'çº¯MADDPG' if algo_name == 'pure_maddpg' else 'çº¯LLM',
                'test_energy': test_results[algo_name].get('energy', 0),
                'test_delay': test_results[algo_name].get('delay', 0),
                'test_utilization': test_results[algo_name].get('utilization', 0),
            }
            comparison_data.append(algo_data)
    
    if not comparison_data:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„å¯¹æ¯”æ•°æ®")
        return
    
    # åˆ›å»ºDataFrameå¹¶ä¿å­˜
    df = pd.DataFrame(comparison_data)
    csv_path = f"{experiment_dir}/comparison/comparison_results.csv"
    df.to_csv(csv_path, index=False, encoding='utf-8-sig')
    
    # æ‰“å°å¯¹æ¯”è¡¨æ ¼
    print("\nğŸ“ˆ ç®—æ³•æ€§èƒ½å¯¹æ¯”ç»“æœ:")
    print("="*120)
    print(df.to_string(index=False, float_format='%.4f'))
    
    # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
    create_comparison_plots(df, experiment_dir)
    
    # ä¿å­˜å®Œæ•´æŠ¥å‘Š
    report = {
        'comparison_data': comparison_data,
        'training_results': training_results,
        'test_results': test_results,
        'experiment_time': datetime.now().isoformat(),
    }
    
    report_file = f"{experiment_dir}/comparison/full_report.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ å®Œæ•´æŠ¥å‘Šä¿å­˜è‡³: {report_file}")
    print(f"ğŸ“Š å¯¹æ¯”è¡¨æ ¼ä¿å­˜è‡³: {csv_path}")


def create_comparison_plots(df, experiment_dir):
    """åˆ›å»ºå¯¹æ¯”å¯è§†åŒ–å›¾è¡¨"""
    print("ğŸ“Š ç”Ÿæˆå¯¹æ¯”å›¾è¡¨...")
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'sans-serif']
    plt.rcParams['axes.unicode_minus'] = False
    
    # åˆ›å»ºå­å›¾
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('LLM+MADDPG vs çº¯MADDPG vs çº¯LLM æ€§èƒ½å¯¹æ¯”', fontsize=16, fontweight='bold')
    
    algorithms = df['algorithm'].tolist()
    colors = ['#2E86AB', '#A23B72', '#F18F01']
    
    # 1. æµ‹è¯•èƒ½è€—å¯¹æ¯”
    if 'test_energy' in df.columns:
        energies = df['test_energy'].tolist()
        axes[0, 0].bar(algorithms, energies, color=colors[:len(algorithms)])
        axes[0, 0].set_title('æµ‹è¯•èƒ½è€—å¯¹æ¯”')
        axes[0, 0].set_ylabel('èƒ½è€— (J)')
        axes[0, 0].tick_params(axis='x', rotation=15)
    
    # 2. æµ‹è¯•æ—¶å»¶å¯¹æ¯”
    if 'test_delay' in df.columns:
        delays = df['test_delay'].tolist()
        axes[0, 1].bar(algorithms, delays, color=colors[:len(algorithms)])
        axes[0, 1].set_title('æµ‹è¯•æ—¶å»¶å¯¹æ¯”')
        axes[0, 1].set_ylabel('æ—¶å»¶ (s)')
        axes[0, 1].tick_params(axis='x', rotation=15)
    
    # 3. èµ„æºåˆ©ç”¨ç‡å¯¹æ¯”
    if 'test_utilization' in df.columns:
        utilizations = df['test_utilization'].tolist()
        axes[1, 0].bar(algorithms, utilizations, color=colors[:len(algorithms)])
        axes[1, 0].set_title('èµ„æºåˆ©ç”¨ç‡å¯¹æ¯”')
        axes[1, 0].set_ylabel('åˆ©ç”¨ç‡')
        axes[1, 0].tick_params(axis='x', rotation=15)
    
    # 4. è®­ç»ƒå¥–åŠ±å¯¹æ¯”ï¼ˆå¦‚æœæœ‰æ•°æ®ï¼‰
    if 'avg_reward' in df.columns:
        rewards = df['avg_reward'].tolist()
        # è¿‡æ»¤æ‰NaNå€¼
        valid_rewards = [(alg, rew) for alg, rew in zip(algorithms, rewards) if not np.isnan(rew)]
        if valid_rewards:
            valid_algs, valid_rews = zip(*valid_rewards)
            axes[1, 1].bar(valid_algs, valid_rews, color=colors[:len(valid_algs)])
            axes[1, 1].set_title('è®­ç»ƒå¹³å‡å¥–åŠ±å¯¹æ¯”')
            axes[1, 1].set_ylabel('å¹³å‡å¥–åŠ±')
            axes[1, 1].tick_params(axis='x', rotation=15)
        else:
            axes[1, 1].text(0.5, 0.5, 'æ— è®­ç»ƒå¥–åŠ±æ•°æ®', ha='center', va='center', transform=axes[1, 1].transAxes)
    else:
        axes[1, 1].text(0.5, 0.5, 'æ— è®­ç»ƒå¥–åŠ±æ•°æ®', ha='center', va='center', transform=axes[1, 1].transAxes)
    
    # è°ƒæ•´å¸ƒå±€
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    plot_path = f"{experiment_dir}/plots/comparison_plots.png"
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"ğŸ“Š å¯¹æ¯”å›¾è¡¨ä¿å­˜è‡³: {plot_path}")


def main():
    """ä¸»å‡½æ•° - ç»Ÿä¸€å…¥å£"""
    parser = argparse.ArgumentParser(description='LLM+MADDPG äº‘è¾¹ç«¯è®¡ç®—å¸è½½ç³»ç»Ÿ')
    parser.add_argument('--mode', type=str, default='all', 
                       choices=['all', 'train', 'test', 'compare', 'llm_maddpg_only', 'maddpg_only', 'llm_only'],
                       help='è¿è¡Œæ¨¡å¼')
    parser.add_argument('--config', type=str, default='config.yaml', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--quick', action='store_true', help='å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼ˆå‡å°‘è®­ç»ƒè½®æ•°ï¼‰')
    parser.add_argument('--no-plots', action='store_true', help='è·³è¿‡å›¾è¡¨ç”Ÿæˆ')
    
    args = parser.parse_args()
    
    # æ‰“å°å¯åŠ¨æ¨ªå¹…
    print_banner()
    print(f"â° å¯åŠ¨æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ¯ è¿è¡Œæ¨¡å¼: {args.mode}")
    
    # åŠ è½½é…ç½®
    try:
        config = load_config(args.config)
        print(f"âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {args.config}")
    except Exception as e:
        print(f"âŒ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
        sys.exit(1)
    
    # å¿«é€Ÿæµ‹è¯•æ¨¡å¼
    if args.quick:
        print("âš¡ å¿«é€Ÿæµ‹è¯•æ¨¡å¼")
        config['training']['episodes'] = 50
        config['training']['max_steps_per_episode'] = 20
    
    # è®¾ç½®éšæœºç§å­
    set_seed(config.get('seed', 42))
    
    # åˆ›å»ºå®éªŒç›®å½•
    experiment_dir = create_result_directories()
    print(f"ğŸ“ å®éªŒç»“æœç›®å½•: {experiment_dir}")
    
    # æ ¹æ®æ¨¡å¼æ‰§è¡Œä¸åŒåŠŸèƒ½
    training_results = {}
    test_results = {}
    
    if args.mode in ['all', 'train', 'llm_maddpg_only']:
        # è®­ç»ƒLLM+MADDPGå®Œæ•´ç‰ˆ
        training_results = train_llm_maddpg_complete(config, experiment_dir)
    
    if args.mode in ['all', 'train']:
        # è®­ç»ƒå…¶ä»–ç®—æ³•
        other_results = train_pure_algorithms(config, experiment_dir)
        training_results.update(other_results)
    
    if args.mode in ['maddpg_only']:
        print("\nğŸ”¥ ä»…è®­ç»ƒçº¯MADDPG")
        try:
            start_time = time.time()
            result = train_maddpg(config)
            training_time = time.time() - start_time
            print(f"âœ… çº¯MADDPGè®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {training_time:.2f}ç§’")
        except Exception as e:
            print(f"âŒ çº¯MADDPGè®­ç»ƒå¤±è´¥: {e}")
    
    if args.mode in ['llm_only']:
        print("\nğŸ§  ä»…è®­ç»ƒçº¯LLM")
        try:
            start_time = time.time()
            result = train_llm(config)
            training_time = time.time() - start_time
            print(f"âœ… çº¯LLMè®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {training_time:.2f}ç§’")
        except Exception as e:
            print(f"âŒ çº¯LLMè®­ç»ƒå¤±è´¥: {e}")
    
    if args.mode in ['all', 'test', 'compare']:
        # è¿è¡Œæµ‹è¯•
        test_results = run_algorithm_tests(config, experiment_dir)
    
    if args.mode in ['all', 'compare'] and (training_results or test_results):
        # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        if not args.no_plots:
            generate_comparison_report(training_results, test_results, experiment_dir)
        else:
            print("â­ï¸ è·³è¿‡å›¾è¡¨ç”Ÿæˆ")
    
    # æ‰“å°æœ€ç»ˆæ€»ç»“
    print("\n" + "ğŸ‰" * 20)
    print("âœ… å®éªŒå®Œæˆï¼")
    print("ğŸ‰" * 20)
    print(f"\nğŸ“ æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {experiment_dir}")
    print(f"ğŸ“Š å¯¹æ¯”æŠ¥å‘Š: {experiment_dir}/comparison/")
    print(f"ğŸ“ˆ å¯è§†åŒ–å›¾è¡¨: {experiment_dir}/plots/")
    
    # ç®€è¦æ˜¾ç¤ºç»“æœ
    if test_results:
        print("\nğŸ“ˆ ç®€è¦æ€§èƒ½å¯¹æ¯”:")
        print("-" * 60)
        for algo, metrics in test_results.items():
            if 'error' not in metrics:
                algo_name = {'llm_maddpg': 'LLM+MADDPG', 'pure_maddpg': 'çº¯MADDPG', 'pure_llm': 'çº¯LLM'}.get(algo, algo)
                print(f"{algo_name:12} | èƒ½è€—: {metrics.get('energy', 0):.4f} | æ—¶å»¶: {metrics.get('delay', 0):.4f}")


if __name__ == "__main__":
    main()